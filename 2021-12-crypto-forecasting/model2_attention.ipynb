{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport os\nimport time\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\nfrom collections import OrderedDict\nimport random as rnd\nrnd.seed(100)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport gresearch_crypto\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport tensorflow as tf\ntf.random.set_seed(200)\nprint('tf version:', tf.__version__)\n\n# Check GPU Availability in Tensorflow\ngpus = tf.config.experimental.list_physical_devices('GPU')\nfor gpu in gpus:\n    print(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)\n\n# List Devices including GPU's with Tensorflow\nfrom tensorflow.python.client import device_lib\ndevice_lib.list_local_devices()\n\n# Check GPU in Tensorflow\ntf.test.is_gpu_available()\n    \n\nFOLDER = os.path.join(os.getcwd(), 'dev')\nif not os.path.isdir(FOLDER):\n    os.mkdir(FOLDER)\n    print('created', FOLDER)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:00:00.207991Z","iopub.execute_input":"2022-01-18T17:00:00.208410Z","iopub.status.idle":"2022-01-18T17:00:03.900362Z","shell.execute_reply.started":"2022-01-18T17:00:00.208295Z","shell.execute_reply":"2022-01-18T17:00:03.899725Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def weighted_correlation(a, b, weights):\n    \n    w = np.ravel(weights)\n    a = np.ravel(a)\n    b = np.ravel(b)\n    \n    sum_w = np.sum(w)\n    mean_a = np.sum(a * w) / sum_w\n    mean_b = np.sum(b * w) / sum_w\n    var_a = np.sum(w * np.square(a - mean_a)) / sum_w\n    var_b = np.sum(w * np.square(b - mean_b)) / sum_w\n    \n    cov = np.sum((a * b * w)) / np.sum(w) - mean_a * mean_b\n    corr = cov / np.sqrt(var_a * var_b)\n    \n    return corr","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:00:03.901730Z","iopub.execute_input":"2022-01-18T17:00:03.902223Z","iopub.status.idle":"2022-01-18T17:00:03.909547Z","shell.execute_reply.started":"2022-01-18T17:00:03.902173Z","shell.execute_reply":"2022-01-18T17:00:03.908530Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"dtype = {'Asset_ID': 'int8', 'Weight': float, 'Asset_Name': str}\nasset_details = pd.read_csv('../input/g-research-crypto-forecasting/asset_details.csv', dtype=dtype)\nasset_details = asset_details.sort_values(by=['Asset_ID']).reset_index(drop=True)\nweights = asset_details['Weight'].values.astype('float32')\nasset_details","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:00:03.910973Z","iopub.execute_input":"2022-01-18T17:00:03.911425Z","iopub.status.idle":"2022-01-18T17:00:03.940572Z","shell.execute_reply.started":"2022-01-18T17:00:03.911383Z","shell.execute_reply":"2022-01-18T17:00:03.939712Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"dtype = {'timestamp': 'int64', 'Asset_ID': 'int8', 'Count': 'int32', \n         'Open': 'float64', 'High': 'float64', 'Low': 'float64', 'Close': 'float64',\n         'Volume': 'float64', 'VWAP': 'float64', 'Target': 'float64'}\n\ndf = pd.DataFrame()\nfor fname in ['train.csv', 'supplemental_train.csv']:\n    df = df.append(pd.read_csv(f'../input/g-research-crypto-forecasting/{fname}', low_memory=False, dtype=dtype))\n\n# [2018-01-01, 2021-09-21], [2021-09-21, 2022-01-10]\ndt = pd.to_datetime(df['timestamp'], unit='s')\nprint([dt.min(), dt.max()])\n\nprint(df.shape)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:00:03.943289Z","iopub.execute_input":"2022-01-18T17:00:03.944095Z","iopub.status.idle":"2022-01-18T17:01:12.749643Z","shell.execute_reply.started":"2022-01-18T17:00:03.944044Z","shell.execute_reply":"2022-01-18T17:01:12.748819Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def time_bounds(dfs, left_shift=0, right_shift=0):\n    start = max([df.index.min() for df in dfs.values()]) + relativedelta(months=left_shift)\n    end = min([df.index.max() for df in dfs.values()]) - relativedelta(months=right_shift)\n    return start, end\n\ndef prep_asset(df_):\n    df = df_.drop('Asset_ID', axis=1).copy()\n    df['datetime'] = pd.to_datetime(df['timestamp'], unit='s')\n    df = df.set_index('datetime')    \n    df = df.sort_index()\n    df = df.loc[~df.index.duplicated()]\n    df[df.isin([np.inf, -np.inf])] = np.nan\n    df = df.reindex( pd.date_range(start=df.index.min(), end=df.index.max(), freq='min') )\n    #df['interpolated'] = df['timestamp'].isnull().astype(float)\n    df = df.interpolate(method='linear', limit_direction='both', axis=0)\n    return df\n\ndfs = OrderedDict([(i, prep_asset(df[df['Asset_ID']==i])) for i in range(14)])\ndel df\n\nstart, end = time_bounds(dfs)\nprint(start, end)\n\ndfs[0].head()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:01:12.750843Z","iopub.execute_input":"2022-01-18T17:01:12.751052Z","iopub.status.idle":"2022-01-18T17:01:50.885289Z","shell.execute_reply.started":"2022-01-18T17:01:12.751026Z","shell.execute_reply":"2022-01-18T17:01:50.884418Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#print(dfs[0].shape)\n#dfs[0].describe().transpose()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:01:50.887120Z","iopub.execute_input":"2022-01-18T17:01:50.887744Z","iopub.status.idle":"2022-01-18T17:01:50.891932Z","shell.execute_reply.started":"2022-01-18T17:01:50.887697Z","shell.execute_reply":"2022-01-18T17:01:50.890957Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def find_outliers(data): \n    # calculate interquartile range\n    q25 = np.percentile(data, 25)\n    q75 = np.percentile(data, 75) \n    iqr = q75 - q25\n    # calculate the outlier cutoff\n    cut_off = iqr * 1.5\n    lb = q25 - cut_off\n    ub = q75 + cut_off    \n    # identify outliers\n    mask = np.logical_or(data < lb, data > ub)\n    return mask, lb, ub\n\n\"\"\"\n# Process Target\nfrom sklearn.preprocessing import MinMaxScaler\n\ntscalers = {i: MinMaxScaler(feature_range=(-1, 1)) for i in range(14)}\n\nfor i in range(14):    \n    target_smoothed = dfs[i]['Target'].ewm(span=5).mean()\n    dfs[i]['Target'] = tscalers[i].fit_transform(target_smoothed.values.reshape(-1,1))\"\"\"\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import RobustScaler\n\ntscalers = {i: MinMaxScaler(feature_range=(-1, 1)) for i in range(14)}\n\nfor i in range(14):  \n    data = dfs[i]['Target']\n    mask, lb, ub = find_outliers(data)\n    data = np.maximum(lb, np.minimum(data, ub))\n    smooth = data.ewm(span=5).mean()\n    dfs[i]['Target'] = tscalers[i].fit_transform(smooth.values.reshape(-1,1))","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:01:50.893531Z","iopub.execute_input":"2022-01-18T17:01:50.894002Z","iopub.status.idle":"2022-01-18T17:01:52.479792Z","shell.execute_reply.started":"2022-01-18T17:01:50.893955Z","shell.execute_reply":"2022-01-18T17:01:52.478763Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#dfs[1][:1000].Target.plot()\n#dfs[1][:1000].Target.ewm(span=5).mean().plot()\n#plt.legend(['original', 'smoothing'], loc='upper right')\n#plt.ylabel('value')\n#plt.xlabel('data')\n\n\"\"\"\nfrom statsmodels.graphics.tsaplots import plot_acf\nfig = plot_acf(dfs[1][:1000].Target, lags=10)\nplt.title(\"Autocorrelation\")\nplt.show()\"\"\"\n#dfs[1][:1000].Target.diff().plot()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:01:52.481325Z","iopub.execute_input":"2022-01-18T17:01:52.482240Z","iopub.status.idle":"2022-01-18T17:01:52.488656Z","shell.execute_reply.started":"2022-01-18T17:01:52.482203Z","shell.execute_reply":"2022-01-18T17:01:52.487886Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def trim_dfs(dfs):\n    start, end = time_bounds(dfs)\n    for i, df in dfs.items():\n        dfs[i] = df[start:]\n\ntrim_dfs(dfs)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:01:52.489711Z","iopub.execute_input":"2022-01-18T17:01:52.489915Z","iopub.status.idle":"2022-01-18T17:01:52.502889Z","shell.execute_reply.started":"2022-01-18T17:01:52.489891Z","shell.execute_reply":"2022-01-18T17:01:52.502262Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#d = dfs[1][:50000]\n#idx = d.index.hour==22 #d.index.weekday==5\n#d[idx].Target.plot()\n\nfor i, df in dfs.items():\n    idx = df.index.minute==30\n    dfs[i] = df[idx]","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:01:52.504093Z","iopub.execute_input":"2022-01-18T17:01:52.504352Z","iopub.status.idle":"2022-01-18T17:01:55.405825Z","shell.execute_reply.started":"2022-01-18T17:01:52.504321Z","shell.execute_reply":"2022-01-18T17:01:55.404913Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_lagged(dfs):\n    for i, df in dfs.items():\n        df_lag = df.drop(['timestamp', 'Target'], axis=1).shift(periods=1).add_suffix('_lag1')\n        dfs[i] = pd.concat([dfs[i], df_lag], axis=1)[1:]\n    return dfs\n\n#dfs = add_lagged(dfs)\n#dfs[0].head(3)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:01:55.407075Z","iopub.execute_input":"2022-01-18T17:01:55.407347Z","iopub.status.idle":"2022-01-18T17:01:55.413251Z","shell.execute_reply.started":"2022-01-18T17:01:55.407314Z","shell.execute_reply":"2022-01-18T17:01:55.412415Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, KBinsDiscretizer\nfrom sklearn.preprocessing import FunctionTransformer\n\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.compose import ColumnTransformer\n\n\ndef timestamp_func(df):\n    \n    ts = df['timestamp'].values\n    \n    day = 24*60*60\n    year = (365.2425)*day\n    \n    arr = np.zeros((ts.size, 4), dtype='float32')\n    arr[:,0] = np.sin(ts * (2 * np.pi / day)) # day_sin\n    arr[:,1] = np.cos(ts * (2 * np.pi / day)) # day_cos\n    arr[:,2] = np.sin(ts * (2 * np.pi / year)) # year_sin\n    arr[:,3] = np.cos(ts * (2 * np.pi / year)) # year_cos    \n    return arr\n\n\ndef extractor_func(df):   \n    eps = 1e-5\n    arr = np.zeros((len(df), 7), dtype='float32')\n    for i, col in enumerate(df.columns):\n        arr[:,i] = np.log(np.maximum(eps, df[col].values))        \n    return arr\n\n\ndef enricher_func(df):    \n    arr = np.zeros((len(df), 5), dtype='float32')    \n    arr[:,0] = df['High'].values - np.maximum(df['Close'].values, df['Open'].values) # Upper_Shadow\n    arr[:,1] = np.minimum(df['Close'].values, df['Open'].values) - df['Low'].values # Lower_Shadow\n    arr[:,2] = df['High'].values - df['Low'].values # spread\n    arr[:,3] = df['Volume'].values/df['Count'].values # mean_trade\n    arr[:,4] = np.log(df['Close'].values/df['Open'].values) # log_price_change\n    return arr\n    \n\ndef build_pipeline():\n    \n    # time features\n    timestamp_trans = FunctionTransformer(func=lambda x: timestamp_func(x))\n    \n    # numerical features\n    extractor_trans = FunctionTransformer(func=lambda x: extractor_func(x))    \n\n    # derived features\n    enricher_trans = FunctionTransformer(func=lambda x: enricher_func(x))\n    \n    # standard scaler\n    #scaler = StandardScaler()\n    \n    numerical_cols = ['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP']\n    lagged_cols = ['Count_lag1', 'Open_lag1', 'High_lag1', 'Low_lag1', 'Close_lag1', 'Volume_lag1', 'VWAP_lag1'] \n    \n    preprocessor = ColumnTransformer([('timestamp', timestamp_trans, ['timestamp']),\n                                      ('numerical', make_pipeline(extractor_trans, StandardScaler()), numerical_cols),\n                                      ('enricher', make_pipeline(enricher_trans, StandardScaler()), numerical_cols),])\n    #                                  ('lagged', make_pipeline(extractor_trans, StandardScaler()), lagged_cols)])    \n    return preprocessor\n\npipelines = {i: build_pipeline().fit(_df.drop('Target', axis=1)) for i, _df in dfs.items()}\nn_features = pipelines[0].transform(dfs[0]).shape[1] - 4\n\n# testing\n#arr = build_pipeline().fit_transform(dfs[1])\n#print(arr.shape)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:01:55.414860Z","iopub.execute_input":"2022-01-18T17:01:55.415297Z","iopub.status.idle":"2022-01-18T17:01:55.848965Z","shell.execute_reply.started":"2022-01-18T17:01:55.415251Z","shell.execute_reply":"2022-01-18T17:01:55.848136Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def data_generator(dfs, start, end, pipelines, batch_size=256, shuffle=True, epochs=1):\n    \"\"\"A data generator function\"\"\"    \n    \n    # create an array with the indexes that can be shuffled\n    indexes = dfs[0][start:end].index.tolist()\n    length = len(indexes)\n    \n    # shuffle the indexes\n    if shuffle:\n        rnd.shuffle(indexes)\n    \n    # init \n    idx = 0 # current location\n    batch_indexes = [0] * batch_size    \n    epoch = 0\n    flag = False\n    \n    targets = np.zeros((batch_size,14), dtype='float32')\n    features = np.zeros((batch_size,14,n_features), dtype='float32')\n    time_encoding = np.zeros((batch_size,14,4), dtype='float32') \n    asset_encoding = np.tile(np.expand_dims(np.eye(14, dtype='float32'), axis=0), (batch_size,1,1))\n    encodings = np.concatenate([time_encoding, asset_encoding], axis=-1)\n    \n    while True:\n        \n        if flag:\n            break\n        \n        for i in range(batch_size):\n            if idx >= length:\n                epoch += 1\n                flag = epoch>=epochs # determine if continue after pass through data\n                idx = 0\n                if shuffle:\n                    rnd.shuffle(indexes)                    \n            batch_indexes[i] = indexes[idx]            \n            idx += 1        \n        \n        for i, df in dfs.items():\n            values = pipelines[i].transform(df.loc[batch_indexes, :])\n            \n            features[:,i,:] = values[:,4:]\n            encodings[:,i,:4] = values[:,:4]\n            \n            targets[:,i] = df.loc[batch_indexes,'Target']\n        \n        inputs = [features, encodings]\n                \n        yield inputs, targets\n\n# testing\nstart, end = time_bounds(dfs)\ngenerator = data_generator(dfs, start, end, pipelines)\ninputs, targets = next(generator)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:01:55.851593Z","iopub.execute_input":"2022-01-18T17:01:55.851845Z","iopub.status.idle":"2022-01-18T17:01:56.334511Z","shell.execute_reply.started":"2022-01-18T17:01:55.851813Z","shell.execute_reply":"2022-01-18T17:01:56.333604Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TransformerBlock(tf.keras.layers.Layer):\n    \"\"\"\n    https://keras.io/examples/nlp/text_classification_with_transformer\n    \"\"\"\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n        super(TransformerBlock, self).__init__()\n        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = tf.keras.Sequential(\n            [tf.keras.layers.Dense(ff_dim, activation=\"relu\"), tf.keras.layers.Dense(embed_dim),]\n        )\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)\n\n\ndef custom_loss(weights):\n    w = tf.constant(weights.reshape(1,-1)/ weights.sum())\n    def wrapper(y_true, y_pred):\n        errors = tf.square(y_true - y_pred) * w\n        #se = tf.reduce_sum(errors, axis=-1)\n        #loss = tf.reduce_mean(se)  \n        loss = tf.reduce_sum(tf.math.reduce_max(errors, axis=0))\n        return loss\n    return wrapper\n\n\ndef build_model(nx, weights):\n\n    features = tf.keras.layers.Input(shape=(14, nx))\n    encodings = tf.keras.layers.Input(shape=(14, 4+14))\n    \n    #embed = tf.keras.layers.Dense(32, activation=\"relu\")(features)\n    #embed = tf.keras.layers.Dropout(0.1)(embed)\n    #embed = tf.keras.layers.Dense(16)(embed)\n\n    #out_attention = TransformerBlock(embed_dim=nx, num_heads=4, ff_dim=64)(features)\n    #out_attention = TransformerBlock(embed_dim=nx, num_heads=4, ff_dim=64)(out_attention)\n    \n    query = tf.keras.layers.Dense(16)(features)\n    value =  tf.keras.layers.Dense(32)(features)\n    key   =  tf.keras.layers.Dense(16)(features)\n    attention = tf.keras.layers.Attention()([query, value, key])\n\n    out_attention = tf.keras.layers.Dropout(0.1)(attention)\n    out_attention = tf.keras.layers.LayerNormalization(epsilon=1e-6)(out_attention)\n\n    hidden = tf.concat([out_attention, encodings], axis=-1)\n\n    dense = tf.keras.layers.Dense(128, activation=\"relu\")(hidden)\n    dense = tf.keras.layers.Dropout(0.1)(dense)\n    outputs = tf.keras.layers.Dense(1)(dense)  \n    outputs = tf.keras.layers.Flatten()(outputs)   \n\n    model = tf.keras.models.Model(inputs=[features, encodings], outputs=outputs)\n    \n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss=custom_loss(weights))\n\n    return model\n\n# build\nmodel = build_model(n_features, weights)\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:26:08.249695Z","iopub.execute_input":"2022-01-18T17:26:08.250052Z","iopub.status.idle":"2022-01-18T17:26:08.426030Z","shell.execute_reply.started":"2022-01-18T17:26:08.250019Z","shell.execute_reply":"2022-01-18T17:26:08.425049Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"#model.predict([np.random.randn(1, 14, 12), np.random.randn(1, 14, 18)]).ravel()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:30:16.539514Z","iopub.execute_input":"2022-01-18T17:30:16.539891Z","iopub.status.idle":"2022-01-18T17:30:16.544060Z","shell.execute_reply.started":"2022-01-18T17:30:16.539851Z","shell.execute_reply":"2022-01-18T17:30:16.543184Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"def train_model(model):\n    \n    train_start, train_end = time_bounds(dfs, right_shift=3)\n    _, val_end = time_bounds(dfs)\n\n    #train_generator = data_generator(dfs, train_start, train_end, pipelines, epochs=10)\n    #val_generator = data_generator(dfs, train_end, val_end, pipelines)    \n    \n    train_n = train_loss = 0\n    val_n = val_loss = 0\n    tic = time.time()    \n    \n    history = {'train_loss': [], 'val_loss': []}\n    epochs = 10\n    step = 0\n        \n    for epoch in range(1,epochs+1):     \n        \n        train_generator = data_generator(dfs, train_start, train_end, pipelines, batch_size=256, shuffle=True)\n        \n        while True:\n            \n            step += 1\n            \n            try:\n                batch_inputs, batch_targets = next(train_generator)\n            except:\n                break\n\n            train_loss += model.train_on_batch(batch_inputs, batch_targets)\n            train_n += 1\n            #print(step, train_loss)\n            if False and step%5000==0:\n                lr = model.optimizer.learning_rate.numpy()*0.75\n                model.optimizer.learning_rate.assign(lr)            \n\n            if step%1000 == 0:      \n                train_loss = np.sqrt(train_loss / train_n)\n                print(f'step: {step} ------------------------')\n                print('train_loss: {:.4f} train_time: {}'.format(train_loss, round((time.time()-tic)/60))) \n                history['train_loss'].append(train_loss)\n                train_n = train_loss = 0\n                tic = time.time()\n\n            if False and step%1000 == 0:            \n                val_generator = data_generator(dfs, train_end, val_end, pipelines, shuffle=False)\n                while True:\n                    try:\n                        batch_inputs, batch_targets = next(val_generator)\n                        batch_predictions = model(batch_inputs)\n                        loss = custom_loss(batch_targets, batch_predictions, w)\n                        val_n += 1\n                        val_loss += loss.numpy()\n                    except:\n                        break\n                val_loss = np.sqrt(val_loss / val_n)\n                print('val_loss: {:.4f} val_time: {}'.format(val_loss, round((time.time()-tic)/60)))           \n                history['val_loss'].append(val_loss)\n                val_n = val_loss = 0\n                tic = time.time()\n            \n    return history\n\nhistory = train_model(model)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:30:16.550472Z","iopub.execute_input":"2022-01-18T17:30:16.551041Z","iopub.status.idle":"2022-01-18T17:32:24.128923Z","shell.execute_reply.started":"2022-01-18T17:30:16.551003Z","shell.execute_reply":"2022-01-18T17:32:24.127993Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"#model.save(f'{FOLDER}/model')\n#model = tf.keras.models.load_model(f'{FOLDER}/model')","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:32:24.130923Z","iopub.execute_input":"2022-01-18T17:32:24.131225Z","iopub.status.idle":"2022-01-18T17:32:24.136562Z","shell.execute_reply.started":"2022-01-18T17:32:24.131171Z","shell.execute_reply":"2022-01-18T17:32:24.135646Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_performance():\n    train_start, train_end = time_bounds(dfs, right_shift=3)\n    _, val_end = time_bounds(dfs)\n\n    generator = data_generator(dfs, train_start, train_end, pipelines, shuffle=False) # train data\n    #generator = data_generator(dfs, train_end, val_end, pipelines, shuffle=False) # val\n\n    generator = data_generator(dfs, train_end, val_end, pipelines)\n\n    batch_inputs, batch_targets = next(generator)\n    batch_predictions = model.predict(batch_inputs)\n\n    i = 1\n    y_pred = batch_predictions[:,i] #tscalers[i].inverse_transform(.reshape(-1,1))\n    y_true = batch_targets[:,i]\n\n    plt.plot(y_pred)\n    plt.plot(y_true)\n    plt.title('model performance')\n    plt.ylabel('value')\n    plt.xlabel('data')\n    plt.legend(['y_pred', 'y_true'], loc='upper right')\n    plt.show()\n\nshow_performance()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:32:24.137771Z","iopub.execute_input":"2022-01-18T17:32:24.138036Z","iopub.status.idle":"2022-01-18T17:32:24.593862Z","shell.execute_reply.started":"2022-01-18T17:32:24.138005Z","shell.execute_reply":"2022-01-18T17:32:24.593073Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nplt.plot(history['train_loss'])\nplt.plot(history['val_loss'])\nplt.title('evolution of loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train_loss', 'val_loss'], loc='upper right')\nplt.show()\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:32:24.595254Z","iopub.execute_input":"2022-01-18T17:32:24.596444Z","iopub.status.idle":"2022-01-18T17:32:24.603941Z","shell.execute_reply.started":"2022-01-18T17:32:24.596395Z","shell.execute_reply":"2022-01-18T17:32:24.602803Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"def make_input(pipelines, df_test, dfs_asset, features, encodings):\n    for _, row in df_test.iterrows():\n        i = int(row['Asset_ID'])  \n        df_asset = dfs_asset[i]\n        for col in df_asset.columns:\n            if row[col]==row[col]:\n                df_asset.loc[0,col] = row[col]\n        # feature transformation\n        values = pipelines[i].transform(df_asset)\n        features[0,i,:] = values[0,4:]\n    encodings[0,:,:4] = np.tile(values[0,:4].reshape(1,4), (14, 1))\n    \n\ndef make_predictions(model, pipelines, dfs):    \n    \n    features = np.zeros((1, 14, n_features), dtype='float32')    \n    time_encoding = np.zeros((1, 14, 4), dtype='float32')\n    asset_encoding = np.expand_dims(np.eye(14, dtype='float32'), axis=0)\n    encodings = np.concatenate([time_encoding, asset_encoding], axis=-1)\n    \n    dfs_asset = {i: df.loc[[df.index[-1]],:].drop('Target', axis=1).copy() for i, df in dfs.items()}\n    values = np.concatenate([pipelines[i].transform(dfs_asset[i]) for i in range(14)], axis=0)\n    \n    features[0,:,:] = values[:,4:]\n\n    env = gresearch_crypto.make_env()\n    iter_test = env.iter_test()\n    \n    for df_test, df_pred in iter_test:  \n        \n        #if datetime.now() < datetime(2022, 2, 3): #(datetime.now()-datetime.fromtimestamp(1642022128)).seconds\n        #    df_pred['Target'] = 0\n        #    env.predict(df_pred)\n        #    continue\n        \n        try:                \n            make_input(pipelines, df_test, dfs_asset, features, encodings)\n\n            predictions = model.predict([features, encodings]).ravel()\n            print('success', predictions) # TODO: tscalers.inverse_transform()\n\n            df_test['Target'] = predictions[df_test['Asset_ID']]            \n            df_pred = df_pred.drop('Target', axis=1).merge(df_test[['row_id', 'Target']], on='row_id', how='left')\n        except:            \n            df_pred['Target'] = 0\n            print('failure')\n        finally:\n            env.predict(df_pred)\n\nmake_predictions(model, pipelines, dfs)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:32:24.606651Z","iopub.execute_input":"2022-01-18T17:32:24.607037Z","iopub.status.idle":"2022-01-18T17:32:24.769118Z","shell.execute_reply.started":"2022-01-18T17:32:24.606990Z","shell.execute_reply":"2022-01-18T17:32:24.767457Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}