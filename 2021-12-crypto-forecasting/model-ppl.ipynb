{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip3 install pyro-ppl","metadata":{"execution":{"iopub.status.busy":"2022-02-08T09:13:27.396611Z","iopub.execute_input":"2022-02-08T09:13:27.397017Z","iopub.status.idle":"2022-02-08T09:13:36.315677Z","shell.execute_reply.started":"2022-02-08T09:13:27.396946Z","shell.execute_reply":"2022-02-08T09:13:36.314577Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport os\nimport time\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\nfrom collections import OrderedDict\nimport random as rnd\nrnd.seed(100)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport gresearch_crypto\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport tensorflow as tf\ntf.random.set_seed(200)\nprint('tf version:', tf.__version__)\n\n# Check GPU Availability in Tensorflow\ngpus = tf.config.experimental.list_physical_devices('GPU')\nfor gpu in gpus:\n    print(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)\n\n# List Devices including GPU's with Tensorflow\nfrom tensorflow.python.client import device_lib\ndevice_lib.list_local_devices()\n\n# Check GPU in Tensorflow\ntf.test.is_gpu_available()\n    \n\nFOLDER = os.path.join(os.getcwd(), 'dev')\nif not os.path.isdir(FOLDER):\n    os.mkdir(FOLDER)\n    print('created', FOLDER)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T09:13:36.320768Z","iopub.execute_input":"2022-02-08T09:13:36.321080Z","iopub.status.idle":"2022-02-08T09:13:42.759907Z","shell.execute_reply.started":"2022-02-08T09:13:36.321047Z","shell.execute_reply":"2022-02-08T09:13:42.759075Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def weighted_correlation(a, b, weights):\n    \n    w = np.ravel(weights)\n    a = np.ravel(a)\n    b = np.ravel(b)\n    \n    sum_w = np.sum(w)\n    mean_a = np.sum(a * w) / sum_w\n    mean_b = np.sum(b * w) / sum_w\n    var_a = np.sum(w * np.square(a - mean_a)) / sum_w\n    var_b = np.sum(w * np.square(b - mean_b)) / sum_w\n    \n    cov = np.sum((a * b * w)) / np.sum(w) - mean_a * mean_b\n    corr = cov / np.sqrt(var_a * var_b)\n    \n    return corr","metadata":{"execution":{"iopub.status.busy":"2022-02-08T09:13:42.761320Z","iopub.execute_input":"2022-02-08T09:13:42.761668Z","iopub.status.idle":"2022-02-08T09:13:42.768691Z","shell.execute_reply.started":"2022-02-08T09:13:42.761636Z","shell.execute_reply":"2022-02-08T09:13:42.767868Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"dtype = {'Asset_ID': 'int8', 'Weight': float, 'Asset_Name': str}\nasset_details = pd.read_csv('../input/g-research-crypto-forecasting/asset_details.csv', dtype=dtype)\nasset_details = asset_details.sort_values(by=['Asset_ID']).reset_index(drop=True)\nweights = asset_details['Weight'].values.astype('float32')\nasset_details","metadata":{"execution":{"iopub.status.busy":"2022-02-08T09:13:42.770591Z","iopub.execute_input":"2022-02-08T09:13:42.771216Z","iopub.status.idle":"2022-02-08T09:13:42.820591Z","shell.execute_reply.started":"2022-02-08T09:13:42.771180Z","shell.execute_reply":"2022-02-08T09:13:42.819870Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"dtype = {'timestamp': 'int64', 'Asset_ID': 'int8', 'Count': 'int32', \n         'Open': 'float64', 'High': 'float64', 'Low': 'float64', 'Close': 'float64',\n         'Volume': 'float64', 'VWAP': 'float64', 'Target': 'float64'}\n\ndf = pd.DataFrame()\nfor fname in ['train.csv', 'supplemental_train.csv']:\n    df = df.append(pd.read_csv(f'../input/g-research-crypto-forecasting/{fname}', low_memory=False, dtype=dtype))\n\ndt = df['timestamp'].astype('datetime64[s]')\nprint([dt.min(), dt.max()])\ndel dt\n\nprint(df.shape)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T09:13:42.821902Z","iopub.execute_input":"2022-02-08T09:13:42.822355Z","iopub.status.idle":"2022-02-08T09:15:00.935756Z","shell.execute_reply.started":"2022-02-08T09:13:42.822322Z","shell.execute_reply":"2022-02-08T09:15:00.934884Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def prep_asset(df_):\n    df = df_.drop('Asset_ID', axis=1).copy()\n    df['datetime'] = df['timestamp'].astype('datetime64[s]')\n    df = df.set_index('datetime')    \n    df = df.sort_index()\n    df = df.loc[~df.index.duplicated()]\n    df[df.isin([np.inf, -np.inf])] = np.nan\n    df = df.reindex( pd.date_range(start=df.index.min(), end=df.index.max(), freq='min') )\n    df = df.interpolate(method='linear', limit_direction='both', axis=0)\n    return df\n\ndfs = OrderedDict([(i, prep_asset(df[df['Asset_ID']==i])) for i in range(14)])\ndel df\ndfs_prod = {i: df.loc[[df.index[-1]]].copy() for i, df in dfs.items()}\n\ndfs[0].head()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T09:15:00.937311Z","iopub.execute_input":"2022-02-08T09:15:00.937817Z","iopub.status.idle":"2022-02-08T09:15:46.026234Z","shell.execute_reply.started":"2022-02-08T09:15:00.937772Z","shell.execute_reply":"2022-02-08T09:15:46.025093Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def time_bounds(dfs, left_shift=0, right_shift=0):\n    start = max([df.index.min() for df in dfs.values()]) + relativedelta(months=left_shift)\n    end = min([df.index.max() for df in dfs.values()]) - relativedelta(months=right_shift)\n    return start, end\n\ndef trim_dfs(dfs):\n    start, end = time_bounds(dfs)\n    print(start, end)\n    for i, df in dfs.items():\n        dfs[i] = df[start:]\n\ntrim_dfs(dfs)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T09:15:46.027574Z","iopub.execute_input":"2022-02-08T09:15:46.027836Z","iopub.status.idle":"2022-02-08T09:15:46.122349Z","shell.execute_reply.started":"2022-02-08T09:15:46.027796Z","shell.execute_reply":"2022-02-08T09:15:46.121432Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def enrich(df, suffix=''):\n    df['Upper_Shadow'+suffix] = df['High'].values / np.maximum(df['Close'].values, df['Open'].values)\n    df['Lower_Shadow'+suffix] =  np.minimum(df['Close'].values, df['Open'].values) / df['Low'].values\n    df['spread'+suffix] = df['High'].values / df['Low'].values\n    #df['mean_trade'+suffix] = df['Volume'].values/df['Count'].values\n    df['price_change'+suffix] = df['Close'].values/df['Open'].values    \n\ndef enrichment(dfs):   \n    for i in range(14):\n        enrich(dfs[i])\n        \nenrichment(dfs)\n\ndfs[1].head(3)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T09:15:46.123970Z","iopub.execute_input":"2022-02-08T09:15:46.124267Z","iopub.status.idle":"2022-02-08T09:15:47.085073Z","shell.execute_reply.started":"2022-02-08T09:15:46.124222Z","shell.execute_reply":"2022-02-08T09:15:47.084050Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def treat_trend(dfs):\n    for i in range(14):\n        df = dfs[i]\n        for col in ['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP']: #, 'mean_trade'\n            s = df[col]\n            s_trans = s/np.maximum(s.shift(1), 1e-5)\n            df[col] = s_trans\n        dfs[i] = df.dropna()\n\ntreat_trend(dfs)\ndfs[1].head(3)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T09:15:47.086419Z","iopub.execute_input":"2022-02-08T09:15:47.087174Z","iopub.status.idle":"2022-02-08T09:15:51.899211Z","shell.execute_reply.started":"2022-02-08T09:15:47.087127Z","shell.execute_reply":"2022-02-08T09:15:51.898169Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def treat_outliers(dfs):\n    bounds = {}\n    columns = ['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP', 'Target'] #, 'mean_trade'\n    columns_q25 = [col+'_q25' for col in columns]\n    columns_q75 = [col+'_q75' for col in columns]\n    columns_join = ['month','hour']\n    \n    for i in range(14):\n        df = dfs[i]\n        \n        df_q25 = df[columns].groupby([df.index.month, df.index.hour]).agg(lambda x: np.percentile(x, 25))   \n        df_q25 = df_q25.add_suffix('_q25')\n        df_q25[columns_join[0]] = df_q25.index.get_level_values(0)\n        df_q25[columns_join[1]] = df_q25.index.get_level_values(1)\n        \n        df_q75 = df[columns].groupby([df.index.month, df.index.hour]).agg(lambda x: np.percentile(x, 75))   \n        df_q75 = df_q75.add_suffix('_q75')\n        df_q75[columns_join[0]] = df_q75.index.get_level_values(0)\n        df_q75[columns_join[1]] = df_q75.index.get_level_values(1)\n        \n        df[columns_join[0]] = df.index.month\n        df[columns_join[1]] = df.index.hour\n        \n        df = df.merge(df_q25, on=columns_join, how='left').merge(df_q75, on=columns_join, how='left')\n        \n        q25 = df[columns_q25].values\n        q75 = df[columns_q75].values\n        iqr = q75 - q25\n        cut_off = iqr * 3\n        lb = q25 - cut_off\n        ub = q75 + cut_off\n        \n        df.loc[:,columns] = np.maximum(lb, np.minimum(df.loc[:,columns].values, ub))\n            \n        df = df.drop(columns_q25+columns_q75+columns_join, axis=1)\n        df['datetime'] = pd.to_datetime(df['timestamp'], unit='s')\n        df = df.set_index('datetime')  \n        dfs[i] = df\n    \n        bounds['lb_{}'.format(i)] = lb\n        bounds['ub_{}'.format(i)] = ub\n        \n    return bounds\n\nbounds = treat_outliers(dfs)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T09:15:51.902022Z","iopub.execute_input":"2022-02-08T09:15:51.902275Z","iopub.status.idle":"2022-02-08T09:17:39.316043Z","shell.execute_reply.started":"2022-02-08T09:15:51.902246Z","shell.execute_reply":"2022-02-08T09:17:39.314098Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def smoothing(dfs):\n    for i in range(14):\n        dfs[i]['Target'] = dfs[i]['Target'].ewm(span=5).mean()\n\nsmoothing(dfs)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T09:17:39.319034Z","iopub.execute_input":"2022-02-08T09:17:39.319521Z","iopub.status.idle":"2022-02-08T09:17:39.675699Z","shell.execute_reply.started":"2022-02-08T09:17:39.319464Z","shell.execute_reply":"2022-02-08T09:17:39.674881Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def target_scaler(dfs):    \n    from sklearn.preprocessing import MinMaxScaler\n    tscalers = {}\n    for i in range(14):  \n        tscalers[i] = MinMaxScaler(feature_range=(-1, 1))\n        values = dfs[i]['Target'].values.reshape(-1,1)        \n        dfs[i]['Target'] = tscalers[i].fit_transform(values)\n    return tscalers\n\ntscalers = target_scaler(dfs)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T09:17:39.677072Z","iopub.execute_input":"2022-02-08T09:17:39.677347Z","iopub.status.idle":"2022-02-08T09:17:40.072251Z","shell.execute_reply.started":"2022-02-08T09:17:39.677315Z","shell.execute_reply":"2022-02-08T09:17:40.071295Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,5))\ndfs[1][:100000].Target.plot()\n#plt.ylabel('value')\n#plt.xlabel('data')\n\n\"\"\"\nfor col in dfs[1].iloc[:,1:]:\n    plt.figure()\n    dfs[1][col].plot()\n    plt.title(col)\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-02-08T09:17:40.073411Z","iopub.execute_input":"2022-02-08T09:17:40.073650Z","iopub.status.idle":"2022-02-08T09:17:41.717900Z","shell.execute_reply.started":"2022-02-08T09:17:40.073623Z","shell.execute_reply":"2022-02-08T09:17:41.716738Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def add_lagged(dfs):\n    for i, df in dfs.items():\n        df_lag = df.drop(['timestamp', 'Target'], axis=1).shift(periods=1).add_suffix('_lag1')\n        dfs[i] =  pd.concat([df, df_lag], axis=1).dropna()\n\nadd_lagged(dfs)\ndfs[1].head(3)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T09:17:41.719357Z","iopub.execute_input":"2022-02-08T09:17:41.719740Z","iopub.status.idle":"2022-02-08T09:17:53.621357Z","shell.execute_reply.started":"2022-02-08T09:17:41.719692Z","shell.execute_reply":"2022-02-08T09:17:53.620240Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#dfs[1].groupby([dfs[1].index.year, dfs[1].index.month])['Target'].mean().plot()\n#pd.Timestamp(1514764860, unit='s').weekday()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T09:17:53.622764Z","iopub.execute_input":"2022-02-08T09:17:53.623014Z","iopub.status.idle":"2022-02-08T09:17:53.627716Z","shell.execute_reply.started":"2022-02-08T09:17:53.622984Z","shell.execute_reply":"2022-02-08T09:17:53.626690Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, KBinsDiscretizer\nfrom sklearn.preprocessing import FunctionTransformer\n\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.compose import ColumnTransformer\n\n\ndef timestamp_func(df):\n    \n    max_month = 12\n    max_weekday = 6\n    max_hour = 23\n    max_minute = 59    \n    max_hour_minute = max_hour*max_minute + max_minute\n\n    month = df.index.month\n    weekday = df.index.weekday\n    hour_minute = df.index.hour*max_minute + df.index.minute\n    \n    arr = np.zeros((len(df), 6), dtype='float32')\n    # month\n    arr[:,0] = np.sin(month * (2 * np.pi / max_month))\n    arr[:,1] = np.cos(month * (2 * np.pi / max_month))\n    # weekday\n    arr[:,2] = np.sin(weekday * (2 * np.pi / max_weekday))\n    arr[:,3] = np.cos(weekday * (2 * np.pi / max_weekday))\n    # hour_minute\n    arr[:,4] = np.sin(hour_minute * (2 * np.pi / max_hour_minute))\n    arr[:,5] = np.cos(hour_minute * (2 * np.pi / max_hour_minute))\n    \n    return arr\n\ncolumns = ['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP']\ncolumns += ['Upper_Shadow', 'Lower_Shadow', 'spread', 'price_change'] #'mean_trade', \ncolumns += [col+'_lag1' for col in columns]\n\npipelines = {'time': FunctionTransformer(func=lambda x: timestamp_func(x))}\nfor i, df in dfs.items():\n    extractor = FunctionTransformer(func=lambda x: x[columns].values)\n    scaler = StandardScaler()\n    pipe = make_pipeline(extractor, scaler)\n    pipelines[i] = pipe.fit(df)\n\nn_cyclic = timestamp_func(dfs[0]).shape[1]\nn_features = pipelines[0].transform(dfs[0]).shape[1]","metadata":{"execution":{"iopub.status.busy":"2022-02-08T09:17:53.629465Z","iopub.execute_input":"2022-02-08T09:17:53.630000Z","iopub.status.idle":"2022-02-08T09:18:03.706476Z","shell.execute_reply.started":"2022-02-08T09:17:53.629950Z","shell.execute_reply":"2022-02-08T09:18:03.705404Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def data_generator(dfs, start, end, pipelines, batch_size=256, shuffle=True, epochs=1):\n    \"\"\"A data generator function. Adapted for single asset\"\"\"    \n    \n    # create an array with the indexes that can be shuffled\n    indexes = dfs[0][start:end].index.tolist()\n    length = len(indexes)\n    \n    # shuffle the indexes\n    if shuffle:\n        rnd.shuffle(indexes)\n    \n    # init \n    idx = 0 # current location\n    batch_indexes = [0] * batch_size    \n    epoch = 0\n    flag = False\n    \n    targets = np.zeros((batch_size,1), dtype='float32')\n    features = np.zeros((batch_size, n_features), dtype='float32')\n    time_encoding = np.zeros((batch_size, n_cyclic), dtype='float32')\n    #targets = np.zeros((batch_size,14), dtype='float32')\n    #features = np.zeros((batch_size,14,n_features), dtype='float32')\n    #time_encoding = np.zeros((batch_size,14,n_cyclic), dtype='float32') \n    #asset_encoding = np.tile(np.expand_dims(np.eye(14, dtype='float32'), axis=0), (batch_size,1,1))\n    #encodings = np.concatenate([time_encoding, asset_encoding], axis=-1)\n    \n    while True:\n        \n        if flag:\n            break\n        \n        for i in range(batch_size):\n            if idx >= length:\n                epoch += 1\n                flag = epoch>=epochs # determine if continue after pass through data\n                idx = 0\n                if shuffle:\n                    rnd.shuffle(indexes)                    \n            batch_indexes[i] = indexes[idx]            \n            idx += 1     \n        \n        i = 1\n        df_batch = dfs[i].loc[batch_indexes]\n        features = pipelines[i].transform(df_batch)\n        time_encoding = pipelines['time'].transform(df_batch)  \n               \n        inputs = np.concatenate([time_encoding, features], axis=-1)\n        targets = df_batch['Target'].values#.reshape(-1,1)\n        \n        yield torch.tensor(inputs, dtype=torch.float), torch.tensor(targets, dtype=torch.float)\n        '''\n        for i, df in dfs.items():\n            df_batch = df.loc[batch_indexes]\n            features[:,i,:] = pipelines[i].transform(df_batch)\n            encodings[:,i,:n_cyclic] = pipelines['time'].transform(df_batch)            \n            targets[:,i] = df_batch['Target']\n        \n        inputs = [features, encodings]'''\n                \n        #yield inputs, targets\n\n# testing\n#start, end = time_bounds(dfs)\n#generator = data_generator(dfs, start, end, pipelines)\n#inputs, targets = next(generator)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T09:18:03.707962Z","iopub.execute_input":"2022-02-08T09:18:03.708206Z","iopub.status.idle":"2022-02-08T09:18:03.722619Z","shell.execute_reply.started":"2022-02-08T09:18:03.708177Z","shell.execute_reply":"2022-02-08T09:18:03.721800Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.distributions import constraints\n\nimport pyro\nimport pyro.distributions as pdist\n\nfrom pyro.nn import PyroModule, PyroSample\nimport torch.nn as nn","metadata":{"execution":{"iopub.status.busy":"2022-02-08T09:37:41.589770Z","iopub.execute_input":"2022-02-08T09:37:41.590914Z","iopub.status.idle":"2022-02-08T09:37:41.597329Z","shell.execute_reply.started":"2022-02-08T09:37:41.590858Z","shell.execute_reply":"2022-02-08T09:37:41.596100Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"def Model(x_dim, y_dim, h_dims=[256, 64]):\n\n    def wrapper(x, y=None):\n        \n        # standard deviation of Normals        \n        sd = 1\n        \n        # Layer 1\n        w1 = pyro.sample(\"w1\", pdist.Normal(0, sd).expand([x_dim, h_dims[0]]).to_event(2))\n        b1 = pyro.sample(\"b1\", pdist.Normal(0, sd).expand([h_dims[0]]).to_event(1))\n        # Layer 2\n        w2 = pyro.sample(\"w2\", pdist.Normal(0, sd).expand([h_dims[0], h_dims[1]]).to_event(2))\n        b2 = pyro.sample(\"b2\", pdist.Normal(0, sd).expand([h_dims[1]]).to_event(1))\n        # Layer 3\n        w3 = pyro.sample(\"w3\", pdist.Normal(0, sd).expand([h_dims[1], y_dim]).to_event(2))\n        b3 = pyro.sample(\"b3\", pdist.Normal(0, sd).expand([y_dim]).to_event(1))       \n        \n        # NN\n        h1 = torch.tanh((x @ w1) + b1)\n        h2 = torch.tanh((h1 @ w2) + b2)\n        mean = (h2 @ w3) + b3\n        \n        sigma = pyro.sample(\"sigma\", pdist.Uniform(0.,1))\n        \n        with pyro.plate(\"targets\", x.shape[0]):\n            obs = pyro.sample(\"obs\", pdist.Normal(mean.squeeze(), sigma), obs=y)\n        \n    return wrapper\n\n'''\nclass Model(PyroModule):\n    # https://www.kaggle.com/carlossouza/simple-bayesian-neural-network-in-pyro\n    def __init__(self, x_dim, h1=128, h2=32):\n        super().__init__()\n        self.fc1 = PyroModule[nn.Linear](x_dim, h1)\n        self.fc1.weight = PyroSample(pdist.Normal(0., 1.).expand([h1, x_dim]).to_event(2))\n        self.fc1.bias = PyroSample(pdist.Normal(0., 1.).expand([h1]).to_event(1))\n        \n        self.fc2 = PyroModule[nn.Linear](h1, h2)\n        self.fc2.weight = PyroSample(pdist.Normal(0., 1.).expand([h2, h1]).to_event(2))\n        self.fc2.bias = PyroSample(pdist.Normal(0., 1.).expand([h2]).to_event(1))\n        \n        self.fc3 = PyroModule[nn.Linear](h2, 1)\n        self.fc3.weight = PyroSample(pdist.Normal(0., 1.).expand([1, h2]).to_event(2))\n        self.fc3.bias = PyroSample(pdist.Normal(0., 1.).expand([1]).to_event(1))\n        self.relu = nn.ReLU()\n\n    def forward(self, x, y=None):\n        #x = x.reshape(-1, 1)\n        h1 = self.relu(self.fc1(x))\n        h2 = self.relu(self.fc2(h1))\n        mu = self.fc3(h2).squeeze()\n        sigma = pyro.sample(\"sigma\", pdist.Uniform(0., 1.))\n        with pyro.plate(\"data\", x.shape[0]):\n            obs = pyro.sample(\"obs\", pdist.Normal(mu, sigma), obs=y)\n        return mu'''\n    \npyro.clear_param_store()\n# Instantiate the Model object\nmodel = Model(x_dim=n_cyclic+n_features, y_dim=1)\n#model = Model(x_dim=n_cyclic+n_features)\nguide = pyro.infer.autoguide.AutoDelta(model)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T09:37:41.600031Z","iopub.execute_input":"2022-02-08T09:37:41.600328Z","iopub.status.idle":"2022-02-08T09:37:41.617355Z","shell.execute_reply.started":"2022-02-08T09:37:41.600295Z","shell.execute_reply":"2022-02-08T09:37:41.616151Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"#model.predict([np.random.randn(1, 14, n_features), np.random.randn(1, 14, n_cyclic+14)]).ravel()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T09:37:41.619302Z","iopub.execute_input":"2022-02-08T09:37:41.619575Z","iopub.status.idle":"2022-02-08T09:37:41.635943Z","shell.execute_reply.started":"2022-02-08T09:37:41.619543Z","shell.execute_reply":"2022-02-08T09:37:41.635201Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"def train_model(model, guide):\n    \n    train_start, train_end = time_bounds(dfs, left_shift=0) #, right_shift=3\n    _, val_end = time_bounds(dfs)\n\n    #train_generator = data_generator(dfs, train_start, train_end, pipelines, epochs=10)\n    #val_generator = data_generator(dfs, train_end, val_end, pipelines)\n    \n    # SVI    \n    svi = pyro.infer.SVI(model,\n                         guide,\n                         pyro.optim.Adam({\"lr\": 1e-3}),\n                         loss=pyro.infer.Trace_ELBO())\n\n    # Clear any previously used parameters\n    pyro.clear_param_store()\n    \n    train_n = train_loss = 0\n    val_n = val_loss = 0\n    tic = time.time()    \n    \n    history = {'train_loss': [], 'val_loss': []}\n    epochs = 10\n    step = 0\n        \n    for epoch in range(1,epochs+1):     \n        \n        train_generator = data_generator(dfs, train_start, train_end, pipelines, batch_size=256, shuffle=True)\n        \n        while True:\n            \n            step += 1\n            \n            try:\n                batch_inputs, batch_targets = next(train_generator)\n            except:\n                break            \n            \n            #train_loss += model.train_on_batch(batch_inputs, batch_targets)\n            train_loss += svi.step(batch_inputs, batch_targets)\n            train_n += 1\n            \n            if False and step%5000==0:\n                lr = model.optimizer.learning_rate.numpy()*0.75\n                model.optimizer.learning_rate.assign(lr)            \n\n            if step%500 == 0:\n                train_loss = np.sqrt(train_loss / train_n)\n                print(f'step: {step} ------------------------')\n                print('train_loss: {:.4f} train_time: {}'.format(train_loss, round((time.time()-tic)/60))) \n                history['train_loss'].append(train_loss)\n                train_n = train_loss = 0\n                tic = time.time()\n\n            if False and step%1000 == 0:            \n                val_generator = data_generator(dfs, train_end, val_end, pipelines, shuffle=False)\n                while True:\n                    try:\n                        batch_inputs, batch_targets = next(val_generator)\n                        batch_predictions = model(batch_inputs)\n                        loss = custom_loss(batch_targets, batch_predictions, w)\n                        val_n += 1\n                        val_loss += loss.numpy()\n                    except:\n                        break\n                val_loss = np.sqrt(val_loss / val_n)\n                print('val_loss: {:.4f} val_time: {}'.format(val_loss, round((time.time()-tic)/60)))           \n                history['val_loss'].append(val_loss)\n                val_n = val_loss = 0\n                tic = time.time()\n            \n    return history\n    \nhistory = train_model(model, guide)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T09:37:41.637525Z","iopub.execute_input":"2022-02-08T09:37:41.638019Z","iopub.status.idle":"2022-02-08T09:56:15.309465Z","shell.execute_reply.started":"2022-02-08T09:37:41.637985Z","shell.execute_reply":"2022-02-08T09:56:15.308403Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"train_start, train_end = time_bounds(dfs, right_shift=3)\n_, val_end = time_bounds(dfs)\n\ngenerator = data_generator(dfs, train_end, val_end, pipelines, shuffle=False, batch_size=1024) # val\nx_test, y_test = next(generator)\ny_true = y_test.detach().numpy().ravel()\n\n# Get the posterior predictive distribution by sampling the model's parameters from the Guide object and applying the model to the test set.\nguide.requires_grad_(False)\n\npredictive = pyro.infer.Predictive(model, guide=guide, num_samples=100, return_sites=[\"obs\"])\npreds = predictive(x_test)\n\nsamples = preds['obs']\n\nfrom pyro.ops.stats import quantile\np10, p50, p90 = quantile(samples, (0.1, 0.5, 0.9)).squeeze(-1)\np10, p50, p90 = p10.detach().numpy().ravel(), p50.detach().numpy().ravel(), p90.detach().numpy().ravel()\n\nfig, ax = plt.subplots(figsize=(10, 5))\nax.plot(y_true, 'o', markersize=1)\nax.plot(p50)\nax.fill_between(y_true, p10, p90, alpha=0.5, color='#ffcd3c');","metadata":{"execution":{"iopub.status.busy":"2022-02-08T09:56:15.311232Z","iopub.execute_input":"2022-02-08T09:56:15.311511Z","iopub.status.idle":"2022-02-08T09:56:16.579285Z","shell.execute_reply.started":"2022-02-08T09:56:15.311461Z","shell.execute_reply":"2022-02-08T09:56:16.578262Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"samples.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-08T09:56:16.582142Z","iopub.execute_input":"2022-02-08T09:56:16.583984Z","iopub.status.idle":"2022-02-08T09:56:16.590354Z","shell.execute_reply.started":"2022-02-08T09:56:16.583925Z","shell.execute_reply":"2022-02-08T09:56:16.589360Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"#model.save(f'{FOLDER}/model')\n#model = tf.keras.models.load_model(f'{FOLDER}/model')","metadata":{"execution":{"iopub.status.busy":"2022-02-08T09:56:16.591900Z","iopub.execute_input":"2022-02-08T09:56:16.592817Z","iopub.status.idle":"2022-02-08T09:56:16.604175Z","shell.execute_reply.started":"2022-02-08T09:56:16.592758Z","shell.execute_reply":"2022-02-08T09:56:16.603479Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"def show_performance():\n    train_start, train_end = time_bounds(dfs, right_shift=3)\n    _, val_end = time_bounds(dfs)\n\n    #generator = data_generator(dfs, train_start, train_end, pipelines, shuffle=False) # train data\n    generator = data_generator(dfs, train_end, val_end, pipelines, shuffle=False, batch_size=1024) # val\n\n    batch_inputs, batch_targets = next(generator)\n    batch_predictions = model.predict(batch_inputs)\n\n    i = 1\n    y_pred = batch_predictions[:,i]\n    y_true = batch_targets[:,i]\n    \n    plt.figure(figsize=(20,10))\n    #plt.plot(y_true-y_pred)\n    plt.plot(y_pred)\n    plt.plot(y_true)\n    plt.title('model performance')\n    plt.ylabel('value')\n    plt.xlabel('data')\n    plt.legend(['y_pred', 'y_true'], loc='upper right')\n    plt.show()\n\nshow_performance()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T09:56:16.605475Z","iopub.execute_input":"2022-02-08T09:56:16.606125Z","iopub.status.idle":"2022-02-08T09:56:17.318874Z","shell.execute_reply.started":"2022-02-08T09:56:16.606076Z","shell.execute_reply":"2022-02-08T09:56:17.317799Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nplt.plot(history['train_loss'])\nplt.plot(history['val_loss'])\nplt.title('evolution of loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train_loss', 'val_loss'], loc='upper right')\nplt.show()\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-02-08T09:56:17.319978Z","iopub.status.idle":"2022-02-08T09:56:17.320322Z","shell.execute_reply.started":"2022-02-08T09:56:17.320136Z","shell.execute_reply":"2022-02-08T09:56:17.320153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}