{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport os\nimport time\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\nfrom collections import OrderedDict\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport gresearch_crypto\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport tensorflow as tf\ntf.random.set_seed(200)\n\nDEVICE = 'CPU' # CPU GPU TPU\n\nif DEVICE == \"TPU\":\n    # detect and init the TPU\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n\n    # instantiate a distribution strategy\n    tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    \n    print('Running on TPU ', tpu.master())\n\n# Check GPU Availability in Tensorflow\ngpus = tf.config.experimental.list_physical_devices('GPU')\nfor gpu in gpus:\n    print(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)\n\n# List Devices including GPU's with Tensorflow\nfrom tensorflow.python.client import device_lib\ndevice_lib.list_local_devices()\n\n# Check GPU in Tensorflow\ntf.test.is_gpu_available()\n    \n\nFOLDER = os.path.join(os.getcwd(), 'dev')\nif not os.path.isdir(FOLDER):\n    os.mkdir(FOLDER)\n    print('created', FOLDER)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T12:24:29.17139Z","iopub.execute_input":"2021-12-06T12:24:29.172076Z","iopub.status.idle":"2021-12-06T12:24:36.061078Z","shell.execute_reply.started":"2021-12-06T12:24:29.171949Z","shell.execute_reply":"2021-12-06T12:24:36.060077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dtype = {'timestamp': np.dtype('int64'), 'Asset_ID': int, 'Count': int, \n         'Open': float, 'High': float, 'Low': float, 'Close': float,\n         'Volume': float, 'VWAP': float, 'Target': float}\n\ndf = pd.read_csv('../input/g-research-crypto-forecasting/train.csv', dtype=dtype)\n\n#df['datetime'] = pd.to_datetime(df['timestamp'], unit='s')\n#df = df.set_index('datetime')#.drop('timestamp', axis=1)\n\nprint(df.shape)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-06T12:24:36.063723Z","iopub.execute_input":"2021-12-06T12:24:36.064219Z","iopub.status.idle":"2021-12-06T12:25:28.425382Z","shell.execute_reply.started":"2021-12-06T12:24:36.064177Z","shell.execute_reply":"2021-12-06T12:25:28.424627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prep_asset_func(df):\n    df['datetime'] = pd.to_datetime(df['timestamp'], unit='s')\n    df = df.set_index('datetime')\n    df = df.sort_index()\n    df = df.loc[~df.index.duplicated()]\n    df[df.isin([np.inf, -np.inf])] = np.nan\n    df = df.reindex( pd.date_range(start=df.index.min(), end=df.index.max(), freq='min') )\n    df['interpolated'] = df['timestamp'].isnull().astype(float)\n    df = df.interpolate(method='linear', limit_direction='both', axis=0)\n    return df\n\ndfs = OrderedDict()\nfor asset_id in sorted(df['Asset_ID'].unique()):    \n    df_asset = df[df['Asset_ID'] == asset_id].copy()   \n    dfs[asset_id] = prep_asset_func(df_asset)\n\ndel df","metadata":{"execution":{"iopub.status.busy":"2021-12-06T12:25:28.426747Z","iopub.execute_input":"2021-12-06T12:25:28.427436Z","iopub.status.idle":"2021-12-06T12:26:01.555533Z","shell.execute_reply.started":"2021-12-06T12:25:28.427396Z","shell.execute_reply":"2021-12-06T12:26:01.554719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfs[1].describe().transpose()","metadata":{"execution":{"iopub.status.busy":"2021-12-06T12:26:01.556888Z","iopub.execute_input":"2021-12-06T12:26:01.557137Z","iopub.status.idle":"2021-12-06T12:26:02.182396Z","shell.execute_reply.started":"2021-12-06T12:26:01.557105Z","shell.execute_reply":"2021-12-06T12:26:02.181551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#for df_ in dfs.values():\n    #print(sum(df_['timestamp'].diff() != 60))\n    #print(df_['timestamp'].diff().max() / 60)\n    #print(df_.interpolated.sum())","metadata":{"execution":{"iopub.status.busy":"2021-12-06T12:26:02.184648Z","iopub.execute_input":"2021-12-06T12:26:02.185467Z","iopub.status.idle":"2021-12-06T12:26:02.189942Z","shell.execute_reply.started":"2021-12-06T12:26:02.185427Z","shell.execute_reply":"2021-12-06T12:26:02.188307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, KBinsDiscretizer\nfrom sklearn.preprocessing import FunctionTransformer\n\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.compose import ColumnTransformer\n\n\ndef timestamp_func(df):\n    \n    day = 24*60*60\n    year = (365.2425)*day\n    \n    timestamp_s = df['timestamp'].values\n\n    day_sin = np.sin(timestamp_s * (2 * np.pi / day)).reshape(-1,1)\n    day_cos = np.cos(timestamp_s * (2 * np.pi / day)).reshape(-1,1)\n    year_sin = np.sin(timestamp_s * (2 * np.pi / year)).reshape(-1,1)\n    year_cos = np.cos(timestamp_s * (2 * np.pi / year)).reshape(-1,1)\n    \n    return np.concatenate([day_sin, day_cos, year_sin, year_cos], axis=1)\n\n\ndef add_features_func(df):\n    \"\"\"A utility function to build features from the original df\"\"\"\n    \n    df['Upper_Shadow'] = df['High'] - np.maximum(df['Close'], df['Open'])\n    df['Lower_Shadow'] = np.minimum(df['Close'], df['Open']) - df['Low']\n    df['spread'] = df['High'] - df['Low']\n    df['mean_trade'] = df['Volume']/df['Count']\n    df['log_price_change'] = np.log(df['Close']/df['Open'])\n    \n    return df\n\ndef values_extractor_func(df):\n    arr = df.values\n    return arr.reshape(len(df), -1)\n    \n\ndef build_pipeline():\n    \n    prep_asset = FunctionTransformer(func=lambda x: prep_asset_func(x))\n    \n    values_extractor = FunctionTransformer(func=lambda x: values_extractor_func(x))\n    \n    # derive new features\n    add_features = FunctionTransformer(func=lambda x: add_features_func(x))\n    \n    # standard scaler\n    scaler = StandardScaler()\n    \n    # discretizer\n    #discretizer = KBinsDiscretizer(n_bins=8, encode='onehot-dense', strategy='kmeans')\n    \n    # time features\n    timestamp_trans = FunctionTransformer(func=lambda x: timestamp_func(x))\n    \n    numlst = ['Open', 'High', 'Low', 'Close', 'Volume', 'VWAP',\n              'Upper_Shadow', 'Lower_Shadow', 'spread', 'mean_trade', 'log_price_change']\n    \n    preprocessor = ColumnTransformer([('timestamp', timestamp_trans, ['timestamp']),                                      \n                                      #('count', discretizer, ['Count']),\n                                      ('numerical', scaler, ['Count']+numlst),\n                                      ('values', values_extractor, ['interpolated'])])  \n    \n    return make_pipeline(prep_asset, add_features, preprocessor)\n\n#preprocessor = build_pipeline()","metadata":{"execution":{"iopub.status.busy":"2021-12-06T12:26:02.191656Z","iopub.execute_input":"2021-12-06T12:26:02.191964Z","iopub.status.idle":"2021-12-06T12:26:02.305598Z","shell.execute_reply.started":"2021-12-06T12:26:02.191927Z","shell.execute_reply":"2021-12-06T12:26:02.304937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#arr = preprocessor.fit_transform(dfs[1])\n#plt.plot(arr[:,2])\n#plt.plot(arr[:,3])\n#plt.xlabel('Time [h]')\n#plt.title('Time of day signal')\n#print(arr.shape)\n#arr[:1]","metadata":{"execution":{"iopub.status.busy":"2021-12-06T12:26:02.306971Z","iopub.execute_input":"2021-12-06T12:26:02.307228Z","iopub.status.idle":"2021-12-06T12:26:02.312191Z","shell.execute_reply.started":"2021-12-06T12:26:02.307192Z","shell.execute_reply":"2021-12-06T12:26:02.311494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model(Tx, nx):   \n    \n    def wrapper(Tx, nx):\n\n        # inputs\n        x = tf.keras.layers.Input(shape=(Tx, nx))\n\n        # layers\n        o1 = tf.keras.layers.GRU(32, return_sequences=True, dropout=0.2)(x)\n        o = tf.keras.layers.GRU(8, return_sequences=True, dropout=0.2)(o1)\n        y = tf.keras.layers.Dense(1, activation='linear')(o)\n\n        # optimizer\n        optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n        \n        model = tf.keras.models.Model(inputs=x, outputs=y)\n        model.compile(loss='mse', optimizer=optimizer) \n        \n        return model, x, o\n\n    # model\n    if DEVICE == \"TPU\":\n        with tpu_strategy.scope():\n            model, x, o = wrapper(Tx, nx)\n    else:\n        model, x, o = wrapper(Tx, nx)   \n\n    return model, x, o\n\n    \ndef create_models(dfs):    \n    models = {}\n    for asset_id, df in dfs.items():\n        model, inp, out = build_model(Tx=20, nx=17)\n        models[asset_id] = {'model': model, 'inp': inp, 'out': out}\n    return models\n\n\ndef create_pipelines(dfs):    \n    pipelines = {}\n    for asset_id, df in dfs.items():\n        pipeline = build_pipeline()        \n        pipelines[asset_id] = pipeline.fit(df)\n    return pipelines    \n    \n\ndef train_model(train_X, train_Y, model, epochs):\n    \n    for epoch in range(epochs):\n        \n        tic = time.time()\n        train_loss = 0\n        \n        # Iterate over the batches of the dataset.\n        for batch_x, batch_y in zip(train_X, train_Y):\n            loss = model.train_on_batch(batch_x, batch_y)\n            train_loss += loss\n            break\n\n        print(\"epoch: {} loss: {:.4f} time: {:.2f}\".format(epoch+1, train_loss, time.time()-tic))\n\n\ndef pretrain(dfs, models, pipelines, end):\n    \n    for asset_id, df in dfs.items():\n        \n        pipeline = pipelines[asset_id]\n        model = models[asset_id]['model']\n        \n        train_df = df[:end]        \n        train_x = pipeline.transform(train_df)\n        train_y = train_df['Target'].values\n        \n        params = {'sequence_length': 20, 'sequence_stride': 1, 'sampling_rate': 1, 'batch_size': 1024, 'shuffle': False}\n        \n        train_X = tf.keras.preprocessing.timeseries_dataset_from_array(data=train_x, targets=None, **params)        \n        train_Y = tf.keras.preprocessing.timeseries_dataset_from_array(data=train_y, targets=None, **params)\n        \n        print('Asset_ID:', asset_id)\n        \n        train_model(train_X, train_Y, model, epochs=1)\n\n\nstart = max([df_.index.min() for df_ in dfs.values()])\nend = min([df_.index.max() for df_ in dfs.values()]) - relativedelta(months=3)\nprint(start, end) \n\n#m, _, _ = build_model(Tx=20, nx=17); m.summary()\n\nmodels = create_models(dfs)\npipelines = create_pipelines(dfs)\n\npretrain(dfs, models, pipelines, end)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T12:26:02.313492Z","iopub.execute_input":"2021-12-06T12:26:02.313881Z","iopub.status.idle":"2021-12-06T12:27:53.519178Z","shell.execute_reply.started":"2021-12-06T12:26:02.313845Z","shell.execute_reply":"2021-12-06T12:27:53.518342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nX = np.arange(100)\n#Y = X*2\nY = np.concatenate([(X*2).reshape(-1,1),(X*2).reshape(-1,1)], axis=1)\n\ninput_dataset = tf.keras.preprocessing.timeseries_dataset_from_array(\n  X, None, batch_size=4, sequence_length=10)\n\ntarget_dataset = tf.keras.preprocessing.timeseries_dataset_from_array(\n  Y, None, batch_size=4, sequence_length=1, start_index=9)\n\nfor inputs, targets in zip(input_dataset, target_dataset):\n    print(inputs)\n    print(targets)\n    break\n\"\"\" \n    \n\"\"\" \ninput_dataset = tf.keras.preprocessing.timeseries_dataset_from_array(\n  X, None, batch_size=4, sequence_length=10)\n\ntarget_dataset = tf.keras.preprocessing.timeseries_dataset_from_array(\n  Y, None, batch_size=4, sequence_length=10)\n\nfor inputs, targets in zip(input_dataset, target_dataset):\n    print(inputs)\n    print(targets)\n    break\"\"\"   \n#print(i)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T12:27:53.521052Z","iopub.execute_input":"2021-12-06T12:27:53.52195Z","iopub.status.idle":"2021-12-06T12:27:53.528792Z","shell.execute_reply.started":"2021-12-06T12:27:53.521906Z","shell.execute_reply":"2021-12-06T12:27:53.528046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef make_train_outputs(dfs, start, end, SEQ_LENGTH=20):\n    \n    lst = [df[start:end]['Target'].values.reshape(-1, 1).astype(np.float32) for df in dfs.values()]\n    \n    data = np.concatenate(lst, axis=1)\n    \n    params = {'sequence_length': 1, 'batch_size': 1024, 'start_index': SEQ_LENGTH-1}        \n    outputs = tf.keras.preprocessing.timeseries_dataset_from_array(data=data, targets=None, **params)\n    \n    return outputs\n\n\ndef make_train_inputs(dfs, pipelines, start, end, SEQ_LENGTH=20):\n    \n    lst = []\n    for asset_id, df in dfs.items():\n        \n        data = pipelines[asset_id].transform(df[start:end])\n        \n        params = {'sequence_length': SEQ_LENGTH, 'batch_size': 1024}        \n        inp = tf.keras.preprocessing.timeseries_dataset_from_array(data=data, targets=None, **params)\n        lst.append(inp)\n    \n    #inputs = zip(*lst)\n    \n    return lst\n\n\nstart = max([df_.index.min() for df_ in dfs.values()])\nend = min([df_.index.max() for df_ in dfs.values()]) - relativedelta(months=3)\nprint('train window:', start, end)  \n\noutputs = make_train_outputs(dfs, start, end)\n#for o in outputs:\n#    print(tf.squeeze(o))\n#    break\n    \ninputs = make_train_inputs(dfs, pipelines, start, end)\n#for batch in inputs:\n#    lst = list(batch)\n#    print(len(lst))\n#    print(lst[0])\n#    break","metadata":{"execution":{"iopub.status.busy":"2021-12-06T12:27:53.530165Z","iopub.execute_input":"2021-12-06T12:27:53.530675Z","iopub.status.idle":"2021-12-06T12:28:13.566547Z","shell.execute_reply.started":"2021-12-06T12:27:53.530636Z","shell.execute_reply":"2021-12-06T12:28:13.565772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_global_model(models):    \n    \n    def wrapper(models):\n    \n        inps = [model['inp'] for model in models.values()]\n        outs = [model['out'] for model in models.values()]\n        x = tf.concat(outs, -1)\n\n        #query = tf.keras.layers.Dense(256)(x)\n        #value =  tf.keras.layers.Dense(256)(x)\n        #key   =  tf.keras.layers.Dense(256)(x)\n        #attention = tf.keras.layers.Attention()([query, value, key])\n        #dense = tf.keras.layers.Flatten()(attention)\n\n        _, dense = tf.keras.layers.GRU(128, return_sequences=False, return_state=True, dropout=0.2)(x)\n\n        y = tf.keras.layers.Dense(14, activation='linear')(dense)\n        \n        model = tf.keras.models.Model(inputs=inps, outputs=y)\n        \n        return model\n    \n    if DEVICE == \"TPU\":\n        with tpu_strategy.scope():\n            model = wrapper(models)\n    else:\n        model = wrapper(models)\n    \n    return model\n\n\ndef loss_fn(y_true, y_pred):\n    squared_difference = tf.square(y_true - y_pred) #* (1 - y_true[1,:])\n    return tf.reduce_mean(squared_difference, axis=-1)\n\n\ndef train_global_model(model, inputs, outputs):\n    \n    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n    \n    dataset = inputs + [outputs]\n\n    for epoch in range(4):\n\n        tic = time.time()\n\n        train_loss = 0\n        \n        step = 0       \n\n        # iterate over the batches\n        for batch in zip(*dataset):\n            \n            batch_x = list(batch[:-1])\n            batch_y = batch[-1]\n\n            with tf.GradientTape() as tape:\n                # compute predictions\n                batch_pred = model(batch_x, training=True)\n                \n                # compute loss\n                errors = tf.square(batch_y - batch_pred)\n                se = tf.reduce_sum(errors, axis=-1)\n                loss = tf.reduce_mean(se)\n\n            grads = tape.gradient(loss, model.trainable_weights)\n            optimizer.apply_gradients(zip(grads, model.trainable_weights))\n            \n            step += 1\n            if step % 10 == 0:\n                print(step, loss.numpy())\n\n            train_loss += loss.numpy()\n            \n            break\n        break\n\n        print(\"epoch: {}, loss: {}, time: {}\".format(epoch, train_loss, time.time()-tic))\n\nmodel = build_global_model(models); model.summary()\n\ntrain_global_model(model, inputs, outputs)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T12:28:13.568167Z","iopub.execute_input":"2021-12-06T12:28:13.568449Z","iopub.status.idle":"2021-12-06T12:28:36.285137Z","shell.execute_reply.started":"2021-12-06T12:28:13.568413Z","shell.execute_reply":"2021-12-06T12:28:36.284237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#filename = './dev/model_v0'\n# Save the weights\n#model.save_weights(filename)\n\n# Create a new model instance\n#model = build_global_model(models)\n\n# Restore the weights\n#model.load_weights(filename)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T12:28:36.286633Z","iopub.execute_input":"2021-12-06T12:28:36.286955Z","iopub.status.idle":"2021-12-06T12:28:36.292732Z","shell.execute_reply.started":"2021-12-06T12:28:36.286917Z","shell.execute_reply":"2021-12-06T12:28:36.291941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def weighted_correlation(a, b, weights):\n    \n    w = np.ravel(weights)\n    a = np.ravel(a)\n    b = np.ravel(b)\n    \n    sum_w = np.sum(w)\n    mean_a = np.sum(a * w) / sum_w\n    mean_b = np.sum(b * w) / sum_w\n    var_a = np.sum(w * np.square(a - mean_a)) / sum_w\n    var_b = np.sum(w * np.square(b - mean_b)) / sum_w\n    \n    cov = np.sum((a * b * w)) / np.sum(w) - mean_a * mean_b\n    corr = cov / np.sqrt(var_a * var_b)\n    \n    return corr","metadata":{"execution":{"iopub.status.busy":"2021-12-06T12:28:36.294203Z","iopub.execute_input":"2021-12-06T12:28:36.294863Z","iopub.status.idle":"2021-12-06T12:28:36.303342Z","shell.execute_reply.started":"2021-12-06T12:28:36.294719Z","shell.execute_reply":"2021-12-06T12:28:36.302532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nhistory = models[0]['hist']\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('evolution of loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'valid'], loc='upper right')\nplt.show()\"\"\"\n\n\"\"\"\nfor asset_id, model in models.items():\n    plt.plot(model['hist'].history['loss'])\n    plt.title('Asset_ID: ' + str(asset_id))\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train'], loc='upper right')\n    plt.show()\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-12-06T12:28:36.306252Z","iopub.execute_input":"2021-12-06T12:28:36.306569Z","iopub.status.idle":"2021-12-06T12:28:36.317286Z","shell.execute_reply.started":"2021-12-06T12:28:36.306527Z","shell.execute_reply":"2021-12-06T12:28:36.315832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_predictions(model, pipelines, dfs, SEQ_LENGTH=20):\n    \n    env = gresearch_crypto.make_env()\n    iter_test = env.iter_test()\n    \n    for df_test, df_pred in iter_test:    \n\n        df_test['datetime'] = pd.to_datetime(df_test['timestamp'], unit='s')\n        df_test = df_test.set_index('datetime').drop('timestamp', axis=1) # shape = (14, 9) \n\n        for i, row in df_test.iterrows():            \n            asset_id = row['Asset_ID']\n            dfs[asset_id].append(df_test.loc[df_test['Asset_ID']==asset_id, :])\n        \n        inputs = []\n        for asset_id, df in dfs.items():\n            \n            tmp_df = df[df.index[-1]-relativedelta(minutes=SEQ_LENGTH-1):]\n                       \n            values = pipelines[asset_id].transform(tmp_df)[-SEQ_LENGTH:]\n            data = np.array([values.tolist()]) # shape = (1, 20, 24)          \n            \n            inputs.append(data)\n        \n        predictions = model.predict(inputs).ravel() # shape = (14, 9)\n        \n        tmp_df = pd.DataFrame({'Asset_ID': range(len(predictions)), 'pred': predictions})\n        df_test = df_test.merge(tmp_df, on=['Asset_ID'], how='left')        \n        df_pred['Target'] = df_pred.merge(df_test[['row_id', 'pred']], on=['row_id'], how='left')['pred']\n        \n        '''\n        for i, row in df_test.iterrows():\n            asset_id = int(row['Asset_ID'])\n            y_pred = predictions[asset_id]\n            df_pred.loc[df_pred['row_id'] == row['row_id'], 'Target'] = y_pred'''\n\n        env.predict(df_pred)\n\ndfs = {asset_id: df[df.index[-1]-relativedelta(months=1):] for asset_id, df in dfs.items()}\nmake_predictions(model, pipelines, dfs)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T12:28:36.318686Z","iopub.execute_input":"2021-12-06T12:28:36.318982Z","iopub.status.idle":"2021-12-06T12:28:44.167174Z","shell.execute_reply.started":"2021-12-06T12:28:36.318944Z","shell.execute_reply":"2021-12-06T12:28:44.166413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}