{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-03T07:42:25.596750Z","iopub.execute_input":"2023-07-03T07:42:25.597146Z","iopub.status.idle":"2023-07-03T07:42:25.605588Z","shell.execute_reply.started":"2023-07-03T07:42:25.597113Z","shell.execute_reply":"2023-07-03T07:42:25.604695Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"/kaggle/input/icr-identify-age-related-conditions/sample_submission.csv\n/kaggle/input/icr-identify-age-related-conditions/greeks.csv\n/kaggle/input/icr-identify-age-related-conditions/train.csv\n/kaggle/input/icr-identify-age-related-conditions/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import os, random, json\nimport itertools\nfrom collections import OrderedDict\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.preprocessing import FunctionTransformer, OneHotEncoder, StandardScaler, PowerTransformer\nfrom sklearn.pipeline import Pipeline, make_pipeline, FeatureUnion\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import make_scorer, accuracy_score\n\nimport seaborn as sns\nsns.set_theme()\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2023-07-03T07:42:25.607363Z","iopub.execute_input":"2023-07-03T07:42:25.607879Z","iopub.status.idle":"2023-07-03T07:42:25.626435Z","shell.execute_reply.started":"2023-07-03T07:42:25.607851Z","shell.execute_reply":"2023-07-03T07:42:25.625200Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_probability as tfp\n\ntfpl = tfp.layers\ntfpd = tfp.distributions\ntfpb = tfp.bijectors\n\nprint('TF version:', tf.__version__)\nprint('TFP version:', tfp.__version__)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T07:42:25.628475Z","iopub.execute_input":"2023-07-03T07:42:25.628854Z","iopub.status.idle":"2023-07-03T07:42:25.638814Z","shell.execute_reply.started":"2023-07-03T07:42:25.628822Z","shell.execute_reply":"2023-07-03T07:42:25.637882Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"TF version: 2.11.0\nTFP version: 0.19.0\n","output_type":"stream"}]},{"cell_type":"code","source":"def build_preprocessor():\n    \"\"\"Only numerical\"\"\"\n    \n    columns = ['AB', 'AF', 'AH', 'AM', 'AR', 'AX', 'AY', 'AZ', 'BC', 'BD ', 'BN', 'BP', 'BQ', 'BR', 'BZ', 'CB', \n                'CC', 'CD ', 'CF', 'CH', 'CL', 'CR', 'CS', 'CU', 'CW ', 'DA', 'DE', 'DF', 'DH', 'DI', 'DL', 'DN', \n                'DU', 'DV', 'DY', 'EB', 'EE', 'EG', 'EH', 'EL', 'EP', 'EU', 'FC', 'FD ', 'FE', 'FI', 'FL', 'FR', \n                'FS', 'GB', 'GE', 'GF', 'GH', 'GI', 'GL']\n    \n    mask = {'0': False, '1': True, '10': True, '11': False, '12': True, '13': False, '14': True, '15': True, '16': True, \n            '17': True, '18': False, '19': True, '2': False, '20': False, '21': False, '22': False, '23': True, '24': False, \n            '25': False, '26': True, '27': True, '28': False, '29': True, '3': True, '30': False, '31': True, '32': True, \n            '33': True, '34': True, '35': True, '36': True, '37': True, '38': False, '39': True, '4': False, '40': True, \n            '41': False, '42': False, '43': False, '44': True, '45': True, '46': True, '47': True, '48': False, '49': True, \n            '5': True, '50': True, '51': True, '52': True, '53': True, '54': False, '6': False, '7': False, '8': True, '9': False}\n    \n    columns = [col for i, col in enumerate(columns) if mask[str(i)]]\n    \n    extractor = FunctionTransformer(lambda df: df[columns].values)\n    \n    #scaler = StandardScaler()\n    scaler = make_pipeline(\n        FunctionTransformer(lambda vals: vals - np.nanmin(vals, axis=0, keepdims=True) + 1),\n        PowerTransformer(method='box-cox', standardize=True)\n    )\n    \n    missing = FunctionTransformer(\n        lambda x: np.concatenate([np.nan_to_num(x), np.isnan(x)], axis=1).astype('float32')\n    )  \n\n    preprocessor  = make_pipeline(extractor, scaler, missing)\n    \n    return preprocessor\n","metadata":{"execution":{"iopub.status.busy":"2023-07-03T07:42:25.640057Z","iopub.execute_input":"2023-07-03T07:42:25.640586Z","iopub.status.idle":"2023-07-03T07:42:25.652053Z","shell.execute_reply.started":"2023-07-03T07:42:25.640558Z","shell.execute_reply":"2023-07-03T07:42:25.651141Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"def prediction_loss(targets, y_pred, eps=1e-5):\n\n    y_true = targets[:,-1:]\n    \n    y = tf.cast(tf.math.maximum(y_true, 0), tf.float32) \n    p = tf.keras.backend.clip(y_pred, eps, 1-eps)\n\n    mask0 = tf.cast(y_true == 0, tf.float32)\n    mask1 = tf.cast(y_true == 1, tf.float32)\n    \n    n0 = tf.math.reduce_sum(mask0) + eps\n    n1 = tf.math.reduce_sum(mask1) + eps\n\n    loss_y0 = tf.math.reduce_sum( mask0 * (1 - y) * tf.math.log(1 - p) ) / n0\n    loss_y1 = tf.math.reduce_sum( mask1 * y * tf.math.log(p) ) / n1\n    loss_nll = -0.5 * (loss_y0 + loss_y1)\n     \n    return loss_nll\n\n\ndef entropy_loss(targets, y_pred, eps=1e-5):\n\n    y_true = targets[:,-1:]\n\n    p = tf.keras.backend.clip(y_pred, eps, 1-eps)\n\n    mask_1 = tf.cast(y_true == -1, tf.float32)    \n\n    n_1 = tf.math.reduce_sum(mask_1) + eps\n    \n    entropy = -p * tf.math.log(p)\n    loss_entropy = tf.math.reduce_sum(mask_1 * entropy) / n_1\n     \n    return loss_entropy","metadata":{"execution":{"iopub.status.busy":"2023-07-03T07:42:25.654111Z","iopub.execute_input":"2023-07-03T07:42:25.654594Z","iopub.status.idle":"2023-07-03T07:42:25.668138Z","shell.execute_reply.started":"2023-07-03T07:42:25.654562Z","shell.execute_reply":"2023-07-03T07:42:25.667245Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"def cutmix_fn(t, pmix=0.7):\n    shape = tf.shape(t)\n    mask = tf.random.uniform(shape) < pmix\n    n = tf.random.uniform([1], minval=0, maxval=shape[1], dtype=tf.dtypes.int32)[0] #-1\n    return tf.where(mask, t, tf.roll(t, -n, axis=0))\n\ndef contrastive_activation(h, h1, tau=0.7):\n    dist = tf.matmul(h, h1, transpose_b=True) / tau\n    dist_scaled = tf.keras.activations.softmax(dist)\n    dist_scaled_diag = tf.linalg.diag_part(dist_scaled)\n    return tf.reshape(dist_scaled_diag, [-1,1])\n\n\nclass SemisupervisedNet(tf.keras.Model):\n    \n    def __init__(self, n_features, n_outputs=1, n_hidden=64, rate=0.3, l2=1e-3, semi_dim=8, pmix=0.7, tau=0.7):\n        super().__init__()             \n\n        self.embedding = tf.keras.layers.Embedding(input_dim=1, output_dim=n_features)        \n        \n        self.dense = tf.keras.layers.Dense(n_hidden, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(l2))\n        self.dropout = tf.keras.layers.Dropout(rate)\n        self.prediction = tf.keras.layers.Dense(n_outputs, activation='sigmoid')\n\n        self.dense1 = tf.keras.layers.Dense(semi_dim)\n        self.cutmix = tf.keras.layers.Lambda(lambda t: cutmix_fn(t, pmix=pmix), name='cutmix')        \n        self.contrastive = tf.keras.layers.Lambda(lambda lst: contrastive_activation(lst[0], lst[1], tau=tau), name='contrastive')  \n\n    def treat_missing(self, x, mask):\n        \"\"\"treat missing values\"\"\"\n        batch_size = tf.shape(x)[0] \n        idx = tf.zeros([batch_size], tf.int32)          \n        x_feat = (1-mask) * x + mask * self.embedding(idx) \n        return x_feat\n        \n    def call(self, inputs):  \n\n        feat, mask = tf.split(inputs, num_or_size_splits=2, axis=1)\n\n        x = self.treat_missing(feat, mask)        \n        x1 = self.cutmix(x)\n\n        h =  self.dense(x)\n        h1 =  self.dense(x1) \n\n        y = self.prediction(self.dropout(h))\n        y1 = self.contrastive([self.dense1(h), self.dense1(h1)])\n\n        outputs = {\n            'prediction': y,\n            'entropy': y,\n            'contrastive': y1,\n        }\n\n        return outputs  ","metadata":{"execution":{"iopub.status.busy":"2023-07-03T07:42:25.695290Z","iopub.execute_input":"2023-07-03T07:42:25.695912Z","iopub.status.idle":"2023-07-03T07:42:25.710870Z","shell.execute_reply.started":"2023-07-03T07:42:25.695879Z","shell.execute_reply":"2023-07-03T07:42:25.709613Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"def fit_semisupervised(params, x_train, y_train, validation_data=None):\n\n    optimizer = tf.optimizers.Adam(learning_rate=params['lr'])\n    \n    loss = {\n        'prediction': prediction_loss,\n        'entropy': entropy_loss,\n        'contrastive': lambda t, p: -tf.math.log(tf.keras.backend.clip(p, 1e-5, 1)),\n    }\n    \n    loss_weights = {key: params[key] for key in ['prediction', 'entropy', 'contrastive']}\n\n    callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_prediction_loss', patience=params['patience'], restore_best_weights=True)]\n\n    model = SemisupervisedNet(n_features=x_train.shape[1]//2, n_hidden=params['n_hidden'], rate=params['rate'], l2=params['l2'])\n\n    model.compile(optimizer=optimizer, loss=loss, loss_weights=loss_weights) \n\n    model.fit(x_train, y_train, validation_data=validation_data, callbacks=callbacks, verbose=params['verbose'], \n                epochs=params['epochs'], batch_size=params['batch_size'])\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-07-03T07:42:25.713209Z","iopub.execute_input":"2023-07-03T07:42:25.714476Z","iopub.status.idle":"2023-07-03T07:42:25.727819Z","shell.execute_reply.started":"2023-07-03T07:42:25.714432Z","shell.execute_reply":"2023-07-03T07:42:25.726916Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"import os, random\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import StratifiedKFold\n\nimport tensorflow as tf\n\n\nPATH_DATA = '/kaggle/input/icr-identify-age-related-conditions/'\nSEED = 4241\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed) \n\n\ndef load_data(folder):\n    \"\"\"Load data\"\"\"\n    greeks_df = pd.read_csv(folder + 'greeks.csv')\n    train_df = pd.read_csv(folder + 'train.csv')    \n    test_df = pd.read_csv(folder + 'test.csv')\n    test_df['Class'] = -1\n    df = pd.concat([train_df, test_df])\n    return df, train_df, test_df, greeks_df   \n\n\ndef get_data():\n    df, train_df, test_df, greeks_df = load_data(PATH_DATA)\n\n    preprocessor = build_preprocessor()\n    XX = preprocessor.fit_transform(df)\n    YY = df[['Class']].values\n\n    X, X_unlabel = XX[:len(train_df)], XX[len(train_df):]\n    Y, Y_unlabel = YY[:len(train_df)], YY[len(train_df):] \n\n    return X, X_unlabel, Y, Y_unlabel\n\n\ndef balanced_log_loss(y_true, y_pred, eps=1e-5): \n    y_true, y_pred = y_true.ravel(), y_pred.ravel()\n    # calculate the number of observations for each class\n    n0, n1 = np.bincount(y_true.astype(int))\n    # clip probabilities    \n    y_pred = np.clip(y_pred, eps, 1-eps)\n    # calculate balanced logarithmic loss\n    log_loss = np.sum((1-y_true) * np.log(1-y_pred)) / n0 + np.sum(y_true * np.log(y_pred)) / n1\n    return -0.5 * log_loss\n\n\nPARAMS = {'epochs': 200, \"patience\": 20, \"verbose\": 0, \n          'prediction': 0.9422988091592843, 'contrastive': 0.3933141976436182, 'entropy': 0,# loss weights\n          'n_hidden': 64, 'batch_size': 28, 'lr': 0.01, 'l2': 0.0012, 'rate': 0.25}\n    \ndef evaluate(params_, X, X_unlabel, Y, Y_unlabel): \n    \"\"\"Computes CV score for semisupervised net\"\"\"\n\n    params = {**PARAMS, **params_}\n\n    tf.keras.backend.clear_session()\n    set_seed(SEED)\n    scores = []    \n    models = []\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=242).split(X, Y.ravel())    \n    for k, (train_index, test_index) in enumerate(skf):\n        \n        x_test = X[test_index]  \n        y_test = Y[test_index]\n        \n        x_train = np.concatenate([X[train_index], X_unlabel, x_test], axis=0)\n        y_train = np.concatenate([Y[train_index], Y_unlabel, -1*np.ones(y_test.shape)], axis=0)\n        \n        model = fit_semisupervised(params, x_train, y_train, validation_data=(x_test, y_test))\n        \n        y_pred = model.predict(x_test)['prediction']\n        \n        score = balanced_log_loss(y_test, y_pred)\n        scores.append(score)\n        \n        models.append(model)\n        \n        #print('Fold:', k, 'Score:', score)\n    #print('CV score:', np.mean(scores))\n\n    return np.mean(scores), models","metadata":{"execution":{"iopub.status.busy":"2023-07-03T07:42:25.729288Z","iopub.execute_input":"2023-07-03T07:42:25.730192Z","iopub.status.idle":"2023-07-03T07:42:25.749098Z","shell.execute_reply.started":"2023-07-03T07:42:25.730128Z","shell.execute_reply":"2023-07-03T07:42:25.748228Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"X, X_unlabel, Y, Y_unlabel = get_data()\nparams = {}\n\nscore, models = evaluate(params, X, X_unlabel, Y, Y_unlabel)\nprint(score)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T07:42:25.750339Z","iopub.execute_input":"2023-07-03T07:42:25.751412Z","iopub.status.idle":"2023-07-03T07:43:04.134312Z","shell.execute_reply.started":"2023-07-03T07:42:25.751369Z","shell.execute_reply":"2023-07-03T07:43:04.133120Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"4/4 [==============================] - 0s 2ms/step\n4/4 [==============================] - 0s 3ms/step\n4/4 [==============================] - 0s 2ms/step\n4/4 [==============================] - 0s 2ms/step\n4/4 [==============================] - 0s 3ms/step\n0.1951446683219618\n","output_type":"stream"}]},{"cell_type":"code","source":"preds = [model.predict(X_unlabel)['prediction'] for model in models]\ny_pred = np.concatenate(preds, axis=1).mean(axis=1, keepdims=True)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T07:43:04.136531Z","iopub.execute_input":"2023-07-03T07:43:04.136845Z","iopub.status.idle":"2023-07-03T07:43:04.491525Z","shell.execute_reply.started":"2023-07-03T07:43:04.136817Z","shell.execute_reply":"2023-07-03T07:43:04.490614Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"test_df = pd.read_csv(PATH_DATA + 'test.csv')\nsubmission = pd.DataFrame(test_df[\"Id\"], columns=[\"Id\"])\nsubmission[\"class_0\"] = 1 - y_pred\nsubmission[\"class_1\"] = y_pred\ndisplay(submission)\n\nsubmission.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T08:06:21.547945Z","iopub.execute_input":"2023-07-03T08:06:21.548355Z","iopub.status.idle":"2023-07-03T08:06:21.568535Z","shell.execute_reply.started":"2023-07-03T08:06:21.548321Z","shell.execute_reply":"2023-07-03T08:06:21.567512Z"},"trusted":true},"execution_count":69,"outputs":[{"output_type":"display_data","data":{"text/plain":"             Id   class_0   class_1\n0  00eed32682bb  0.999634  0.000366\n1  010ebe33f668  0.999634  0.000366\n2  02fa521e1838  0.999634  0.000366\n3  040e15f562a2  0.999634  0.000366\n4  046e85c7cc7f  0.999634  0.000366","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>class_0</th>\n      <th>class_1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00eed32682bb</td>\n      <td>0.999634</td>\n      <td>0.000366</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>010ebe33f668</td>\n      <td>0.999634</td>\n      <td>0.000366</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>02fa521e1838</td>\n      <td>0.999634</td>\n      <td>0.000366</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>040e15f562a2</td>\n      <td>0.999634</td>\n      <td>0.000366</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>046e85c7cc7f</td>\n      <td>0.999634</td>\n      <td>0.000366</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}