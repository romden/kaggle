{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":57891,"databundleVersionId":7056235,"sourceType":"competition"},{"sourceId":7174831,"sourceType":"datasetVersion","datasetId":4145968}],"dockerImageVersionId":30615,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-11T10:36:10.887710Z","iopub.execute_input":"2023-12-11T10:36:10.888154Z","iopub.status.idle":"2023-12-11T10:36:11.400794Z","shell.execute_reply.started":"2023-12-11T10:36:10.888121Z","shell.execute_reply":"2023-12-11T10:36:11.399675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PATH = '/kaggle/input/submission-03'","metadata":{"execution":{"iopub.status.busy":"2023-12-11T10:36:11.402483Z","iopub.execute_input":"2023-12-11T10:36:11.403878Z","iopub.status.idle":"2023-12-11T10:36:11.413158Z","shell.execute_reply.started":"2023-12-11T10:36:11.403796Z","shell.execute_reply":"2023-12-11T10:36:11.411901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport os\nfrom collections import OrderedDict, Counter\nimport itertools\nimport pickle\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline, make_pipeline, FeatureUnion\nfrom sklearn.preprocessing import FunctionTransformer, StandardScaler, PowerTransformer, MinMaxScaler, OneHotEncoder, PolynomialFeatures\nfrom sklearn.impute import SimpleImputer, KNNImputer\n\nCOLUMNS = {\n    'target': ['target'],\n    'time': ['seconds_in_bucket'],\n    'cat': ['imbalance_buy_sell_flag'],\n    'num': ['imbalance_size', 'matched_size', 'reference_price', 'far_price', 'near_price', 'bid_price', 'bid_size', 'ask_price', 'ask_size', 'wap'],\n    'aug': ['ratio_imbalance_size_matched_size', 'ratio_far_price_reference_price', 'ratio_near_price_reference_price', 'ratio_bid_price_ask_price', 'ratio_bid_size_ask_size'],\n}\n\n\ndef feature_augmentation(df):\n    df['ratio_imbalance_size_matched_size'] = df['imbalance_size'] / (df['matched_size'] + 1)\n    df['ratio_far_price_reference_price'] = df['far_price'] / (df['reference_price'] + 1)\n    df['ratio_near_price_reference_price'] = df['near_price'] / (df['reference_price'] + 1)\n    df['ratio_bid_price_ask_price'] = df['bid_price'] / (df['ask_price'] + 1)\n    df['ratio_bid_size_ask_size'] = df['bid_size'] / (df['ask_size'] + 1)\n    return df\n\n\ndef create_dataframes(df):\n    \n    if 'target' not in df.columns:\n        df['target'] = 0\n\n    featcols = ['stock_id'] + COLUMNS['time'] + COLUMNS['cat'] + COLUMNS['num'] +  COLUMNS['aug']\n    \n    grouped = df.groupby(['date_id', 'seconds_in_bucket'])\n    stock_id_df = pd.DataFrame(np.arange(200)[:,None], columns=['stock_id'])\n\n    features = np.zeros((len(grouped), 200, len(featcols)), dtype='float32')\n    targets = np.zeros((len(grouped), 200), dtype='float32')\n\n    for i, (key, group) in enumerate(grouped):   \n        date_id, seconds_in_bucket = key\n        group_df = stock_id_df.merge(group, on='stock_id', how='left')\n        group_df['seconds_in_bucket'] = seconds_in_bucket\n        \n        features[i,:,:] = group_df[featcols].values\n        targets[i,:] = group_df['target'].values\n    \n    features_dfs = [pd.DataFrame(features[:,i,:], columns=featcols) for i in range(200)]\n    targets_df = pd.DataFrame(targets) \n\n    return features_dfs, targets_df\n\n\nclass NumericalTransformer(BaseEstimator, TransformerMixin):\n        \n    def fit(self, X, y=None):\n        values = X.values if isinstance(X, pd.DataFrame) else X\n        self.lb = np.nanmin(values, axis=0, keepdims=True)\n        self.ub = np.nanmax(values, axis=0, keepdims=True)\n        self.mean = np.nanmean(values, axis=0, keepdims=True)\n        return self\n\n    def transform(self, X, y=None):\n        values = X.values if isinstance(X, pd.DataFrame) else X\n        values = np.nan_to_num(values) + np.isnan(values) * self.mean\n        values = np.clip(values, self.lb, self.ub) - self.lb\n        values = np.log1p(values) + 1\n        return values\n    \n\ndef cyclic_encoding(vals, max_val=540):\n    \"\"\"Computes cyclic feature for a given array and max possible value.\"\"\"\n    x = 2* np.pi * vals.ravel() / max_val\n    arr = np.empty((len(x), 2), dtype='float32')\n    arr[:,0] = np.sin(x)\n    arr[:,1] = np.cos(x)\n    return arr\n\n\nclass Enricher:\n\n    def __init__(self):\n        pass\n\n    def feature_aggregation(self, df, keys):\n        cols = COLUMNS['cat']\n        funcs = [lambda s: Counter(s).most_common(1)[0][0]]\n        df_cat = df.groupby(keys)[cols].agg(funcs)\n        df_cat.columns = ['_'.join(names).strip() for names in itertools.product(cols, ['mode'])]\n\n        cols = COLUMNS['num'] + COLUMNS['aug'] + COLUMNS['target']\n        funcs = ['mean', 'std', 'min', 'max']\n        df_num = df.groupby(keys)[cols].agg(funcs)\n        df_num.columns = ['_'.join(names).strip() for names in itertools.product(cols, funcs)]\n\n        df_all = df_cat.merge(df_num, on=keys, how='left')\n        return df_all \n    \n    def load(self):\n        self.enrichment = {}\n        for key in ['agg_stock', 'agg_stock_seconds']:\n            self.enrichment[key] = pd.read_csv(os.path.join(PATH, 'feature_store', f'{key}.csv'))\n        return self\n    \n    def save(self):\n        for key, df in self.enrichment.items():\n            df.to_csv(os.path.join(PATH, 'feature_store', f'{key}.csv'))\n        return self\n\n    def fit(self, df):        \n        self.enrichment = {\n            'agg_stock': self.feature_aggregation(df, keys=['stock_id']).add_suffix('_agg_stock'),\n            'agg_stock_seconds': self.feature_aggregation(df, keys=['stock_id', 'seconds_in_bucket']).add_suffix('_agg_stock_seconds'),\n        }         \n        return self\n    \n    def transform(self, dfs):        \n        for i, df in enumerate(dfs):\n            df = df.merge(self.enrichment['agg_stock'], on=['stock_id'], how='left') #, suffixes=(None, '_agg_stock')\n            df = df.merge(self.enrichment['agg_stock_seconds'], on=['stock_id', 'seconds_in_bucket'], how='left') #, suffixes=(None, '_agg_stock_seconds')\n            dfs[i] = df\n        return dfs\n\n\nclass Preprocessor:\n\n    def __init__(self):\n        pass\n\n    def create_transformer(self, enrichcols):\n\n        pipecols = {\n            'time': COLUMNS['time'],\n            'cat': COLUMNS['cat'],\n            'isnan': COLUMNS['num'] + COLUMNS['aug'],\n            'num': COLUMNS['num'] + COLUMNS['aug'] + enrichcols,\n        }\n\n        pipetrans = {\n            'time': make_pipeline(FunctionTransformer(np.nan_to_num), FunctionTransformer(cyclic_encoding)),\n            'cat': FunctionTransformer(np.nan_to_num),\n            'isnan': FunctionTransformer(np.isnan),\n            'num': make_pipeline(NumericalTransformer(), StandardScaler()), #PowerTransformer(method='box-cox', standardize=True)\n        }\n\n        featkeys = ['time', 'isnan', 'cat', 'num']\n\n        transformer = ColumnTransformer([(key, pipetrans[key], pipecols[key]) for key in featkeys])\n        return transformer\n\n    def fit(self, features_dfs, targets_df):\n        self.transformers = {'targets': StandardScaler().fit(targets_df)}\n        enrichcols1 = [col for col in features_dfs[0].columns if col.endswith('_agg_stock')]\n        enrichcols2 = [col for col in features_dfs[0].columns if col.endswith('_agg_stock_seconds')]\n        enrichcols = enrichcols1 + enrichcols2\n        for i, stock_df in enumerate(features_dfs):\n            self.transformers[f'stock_{i}'] = self.create_transformer(enrichcols).fit(stock_df)\n        return self\n\n    def transform_features(self,  features_dfs, targets_df=None):\n        data = []\n        for i, stock_df in enumerate(features_dfs):\n            x = self.transformers[f'stock_{i}'].transform(stock_df)\n            data.append(x[:,None,:]) \n        feats = np.concatenate(data, axis=1)\n        return feats\n    \n    def transform_targets(self, targets_df):\n        y = self.transformers['targets'].transform(targets_df)\n        return np.concatenate([np.nan_to_num(y)[:,:,None], (1 - np.isnan(y))[:,:,None]], axis=2)\n    \n    def inverse_transform_targets(self, y):\n        return self.transformers['targets'].inverse_transform(y)\n\n\nenricher = Enricher().load() \npreprocessor = pickle.load( open(os.path.join(PATH, 'preprocessor.pkl'), 'rb') ) ","metadata":{"execution":{"iopub.status.busy":"2023-12-11T10:36:11.415273Z","iopub.execute_input":"2023-12-11T10:36:11.415761Z","iopub.status.idle":"2023-12-11T10:36:12.585990Z","shell.execute_reply.started":"2023-12-11T10:36:11.415715Z","shell.execute_reply":"2023-12-11T10:36:12.584976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)\ntf.random.set_seed(4243) \n\nclass AttentionBlock(tf.keras.layers.Layer):\n    \n    def __init__(self, num_heads, n_inputs):\n        super().__init__()        \n        self.blocks = []\n        dims = [n_inputs // num_heads for _ in range(num_heads)]\n        dims[-1] += n_inputs - sum(dims)\n        for att_dims in dims:\n            block = {\n                'query': tf.keras.layers.Dense(att_dims, use_bias=False),\n                'value': tf.keras.layers.Dense(att_dims, use_bias=False),\n                'attention': tf.keras.layers.Attention(use_scale=True),\n            }\n            self.blocks.append(block)\n        self.ffn = tf.keras.Sequential([\n            tf.keras.layers.Dense(512, activation='relu'), #, kernel_regularizer=tf.keras.regularizers.L2(1e-4)\n            tf.keras.layers.Dense(32),            \n        ])\n        self.layernorm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n    def call(self, inputs):\n        lst = []\n        for block in self.blocks:\n            query = block['query'](inputs)\n            value = block['value'](inputs)\n            att_out = block['attention']([query, value])\n            lst.append(att_out)\n        att = tf.concat(lst, axis=2)\n        h = self.layernorm(att + inputs)\n        outputs = self.ffn(h)\n        return outputs \n    \n    \nclass Model(tf.keras.Model):\n\n    def __init__(self, n_inputs):\n        super().__init__() \n        \n        self.attention = AttentionBlock(num_heads=6, n_inputs=n_inputs)                \n        self.conv = tf.keras.layers.Conv1D(filters=256, kernel_size=200)\n        self.flatten = tf.keras.layers.Flatten()\n\n        self.ffn = tf.keras.Sequential([\n            tf.keras.layers.BatchNormalization(),\n            tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(1e-4)),\n            tf.keras.layers.Dense(200),\n        ])\n\n    def call(self, inputs):        \n        att = self.attention(inputs)\n        h = self.conv(att)      \n        h = self.flatten(h)\n        outputs = self.ffn(h)\n        return outputs  \n\nn_inputs = 163\nmodel = Model(n_inputs)\nmodel.load_weights( os.path.join(PATH, 'model', 'weights') )\nmodel.predict(np.zeros((1,200,n_inputs), dtype='float32'));","metadata":{"execution":{"iopub.status.busy":"2023-12-11T10:36:12.588157Z","iopub.execute_input":"2023-12-11T10:36:12.588760Z","iopub.status.idle":"2023-12-11T10:36:30.129415Z","shell.execute_reply.started":"2023-12-11T10:36:12.588722Z","shell.execute_reply":"2023-12-11T10:36:30.128445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import optiver2023\nenv = optiver2023.make_env()\niter_test = env.iter_test()\n\n\ndef helper_features(test):\n    df = feature_augmentation(test) \n    features_dfs, targets_df = create_dataframes(df) \n    enriched_features_dfs = enricher.transform(features_dfs)    \n    features = preprocessor.transform_features(enriched_features_dfs)\n    return features\n\ndef helper_predictions(y_pred):\n    y_pred = preprocessor.inverse_transform_targets(y_pred)\n    y_pred = y_pred.ravel()\n    return y_pred    \n\n#cache = pd.DataFrame()\n\nfor (test, revealed_targets, sample_prediction) in iter_test:\n    \n    features = helper_features(test)\n    \n    y_pred = model.predict(features)\n    \n    y_pred = helper_predictions(y_pred)\n    \n    sample_prediction['target'] = sample_prediction['row_id'].apply( lambda z: y_pred[int(z.split(\"_\")[-1])] )\n    \n    env.predict(sample_prediction)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T10:36:30.130932Z","iopub.execute_input":"2023-12-11T10:36:30.131287Z","iopub.status.idle":"2023-12-11T10:42:28.320121Z","shell.execute_reply.started":"2023-12-11T10:36:30.131258Z","shell.execute_reply":"2023-12-11T10:42:28.317634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}